{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "DDJwQPZcupab",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "**Goethe University Frankfurt am Main**\n",
    "\n",
    "Winter Semester 2022/23\n",
    "\n",
    "<br>\n",
    "\n",
    "## *Assignment 5 (Two-Stage Object Detector)*\n",
    "\n",
    "---\n",
    "\n",
    "**Points:** 50<br>\n",
    "**Due:** 30.11.2022, 10 am<br>\n",
    "**Contact:** Timothy Schaumlöffel (Discord)<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Your Name:** Tilo-Lars Flasche\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egMvFYeolUEz"
   },
   "source": [
    "<br>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "---\n",
    "\n",
    "- [0 Introduction](#0-Introduction)\n",
    "- [1 Dataset](#1-Dataset)\n",
    "- [2 FPN + RPN](#2-FPN-+-RPN-(5-Points))\n",
    "  - [2.1 Backbone with Feature Pyramid Networks (FPN)](#2.1-Backbone-with-Feature-Pyramid-Networks-(FPN))\n",
    "  - [2.2 Faster R-CNN first stage: Region Proposal Network (RPN)](#2.2-Faster-R-CNN-first-stage:-Region-Proposal-Network-(RPN))\n",
    "- [3 Training Target RPN](#3-Training-Target-RPN-(25-Points))\n",
    " - [3.1 Anchor-based Training of RPN](#3.1-Anchor-based-Training-of-RPN)\n",
    " - [3.2 Loss Functions](#3.2-Loss-Functions)\n",
    " - [3.3 Putting it all together: RPN module](#3.3-Putting-it-all-together:-RPN-module)\n",
    "- [4 Faster R-CNN](#4-Faster-R-CNN-(20-Points))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "PoRTyUc94S1a",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "# 0 Introduction\n",
    "\n",
    "---\n",
    "\n",
    "## Faster R-CNN: A Classic Two-Stage Anchor-Based Object Detector\n",
    "\n",
    "In this exercise you will implement a **two-stage** object detector, based on [Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf), which consists of two modules - Region Proposal Networks (RPN) and Fast R-CNN.\n",
    "Like one-stage detector in the first part of our assignment,\n",
    "we will train it to detect a set of object classes and evaluate the detection accuracy using the classic metric mean Average Precision ([mAP](https://github.com/Cartucho/mAP))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "LfBk3NtRgqaV",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "ubB_0e-UAOVK",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Setup Code\n",
    "Before getting started, we need to run some boilerplate code to set up our environment, same as previous assignments. You'll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "id": "ASkY27ZtA7Is",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYrTb8TMlUE7",
    "outputId": "359cc04d-598a-4ed5-f04d-2baaf27a2b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "fatal: destination path 'mAP' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# To download the dataset\n",
    "!pip install wget\n",
    "# for mAP evaluation\n",
    "!git clone https://github.com/Cartucho/mAP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "MzqbYcKdz6ew",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Google Colab Setup\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "HzRdJ3uhe1CR",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "7ecf673f-e795-4fe9-f12b-aa8ab64c099b",
    "run_control": {
     "read_only": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "OvUDZWGU3VLV",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the following cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "[\"common.py\", \"one_stage_detector.ipynb\", \"two_stage_detector.ipynb\", \"one_stage_detector.py\", \"two_stage_detector.py\", \"utils\", \"data\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "RrAX9FOLpr9k",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "233e5b45-4f05-4fa8-aab3-4fd48343432e",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Das System kann den angegebenen Pfad nicht finden: 'drive\\\\My Drive'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m GOOGLE_DRIVE_PATH_AFTER_MYDRIVE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m ROOT_PATH \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy Drive\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m GOOGLE_DRIVE_PATH_AFTER_MYDRIVE\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mROOT_PATH\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Add to sys so we can import .py files.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(ROOT_PATH))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\pathlib.py:1017\u001b[0m, in \u001b[0;36mPath.iterdir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;124;03m\"\"\"Iterate over the files in this directory.  Does not yield any\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;124;03m    result for the special paths '.' and '..'.\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1017\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m}:\n\u001b[0;32m   1019\u001b[0m             \u001b[38;5;66;03m# Yielding a path object for these makes little sense\u001b[39;00m\n\u001b[0;32m   1020\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Das System kann den angegebenen Pfad nicht finden: 'drive\\\\My Drive'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a 2022WI folder and put all the files under A5 folder, then \"2022WI/A4\"\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = ''\n",
    "ROOT_PATH = Path(\"drive\", \"My Drive\") / GOOGLE_DRIVE_PATH_AFTER_MYDRIVE\n",
    "print(list(ROOT_PATH.iterdir()))\n",
    "\n",
    "# Add to sys so we can import .py files.\n",
    "sys.path.append(str(ROOT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fpcmatIhlUFA"
   },
   "outputs": [],
   "source": [
    "# If you run locally set root path project path\n",
    "#from pathlib import Path\n",
    "ROOT_PATH = Path(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "RldDumJE48pv",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### General Setup\n",
    "\n",
    "Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
    "\n",
    "```\n",
    "Hello from common.py!\n",
    "Hello from two_stage_detector.py!\n",
    "Hello from helper.py!\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "collapsed": false,
    "id": "pTIwSpkS495_",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "915e314b-1deb-4ad3-83ca-672f13c82c53",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from common.py!\n",
      "Hello from two_stage_detector.py!\n",
      "Hello from helper.py!\n"
     ]
    }
   ],
   "source": [
    "from common import hello_common\n",
    "from two_stage_detector import hello_two_stage_detector\n",
    "from utils.helper import hello_helper\n",
    "\n",
    "\n",
    "hello_common()\n",
    "hello_two_stage_detector()\n",
    "hello_helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "GWP1vCGL5Eca",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Load several useful packages that are used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "collapsed": false,
    "id": "CwVZ26yM5G8U",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from utils.helper import rel_error, reset_seed\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# for plotting\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "x7poKGI35JZY",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "collapsed": false,
    "id": "Vw3wIuCu5LnU",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please set GPU via Edit -> Notebook Settings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:83: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Good to go!\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "MjJ3uyYBg3Lw",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "# 1 Dataset\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Load PASCAL VOC 2007 data\n",
    "As in the previous notebook, we will use PASCAL VOC 2007 dataset to train our model. The following two cells are exactly same as those in `one_stage_detector.ipynb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PvqnOa8LlUFF"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Set a few constants related to data loading.\n",
    "NUM_CLASSES = 20\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SHAPE = (224, 224)\n",
    "NUM_WORKERS = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "MmEP5KQJzk0d",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "61647dbf-69c7-47ec-bec7-8b7e7055b265",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC2007DetectionTiny(train): size=2501\n",
      "VOC2007DetectionTiny(val): size=2510\n"
     ]
    }
   ],
   "source": [
    "from utils.dataset import VOC2007DetectionTiny\n",
    "\n",
    "\n",
    "# If this hangs, download and place the data in your drive manually.\n",
    "train_dataset = VOC2007DetectionTiny(dataset_dir=ROOT_PATH / \"data\", \n",
    "                                     split=\"train\", \n",
    "                                     image_size=IMAGE_SHAPE[0])\n",
    "print(train_dataset)\n",
    "\n",
    "val_dataset = VOC2007DetectionTiny(dataset_dir=ROOT_PATH / \"data\",\n",
    "                                   split=\"val\", \n",
    "                                   image_size=IMAGE_SHAPE[0])\n",
    "\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLTOQO5olUFG"
   },
   "source": [
    "Now we wrap these dataset objects with PyTorch dataloaders, similar to `one_stage_detector.ipynb`. The format of output batches will also be same as what you have seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "eWOmQVNslUFG",
    "outputId": "a47f5746-a2e4-4861-ecbb-9e0cb22c4d36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path has len : 16\n",
      "images has shape : torch.Size([16, 3, 224, 224])\n",
      "gt_boxes has shape    : torch.Size([16, 40, 5])\n",
      "Five boxes per image  :\n",
      "tensor([[[ 48.9369,  65.2493, 180.1081, 181.6216,   6.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n",
      "\n",
      "        [[ 78.8462,  38.1538, 136.6923, 122.4615,  14.0000],\n",
      "         [ 20.3846,  48.0000, 213.0000, 206.7692,  12.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n",
      "\n",
      "        [[  6.0359,  99.2515, 164.3114, 224.0000,   1.0000],\n",
      "         [154.2515,  92.5449, 224.0000, 224.0000,   1.0000],\n",
      "         [  1.3413,   0.0000,  78.4671, 192.4731,  14.0000],\n",
      "         [  2.0120,   0.0000, 162.9701, 224.0000,  14.0000],\n",
      "         [150.8982,   0.0000, 224.0000, 224.0000,  14.0000]],\n",
      "\n",
      "        [[  4.5405,  84.0841, 170.6907, 142.6066,   6.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n",
      "\n",
      "        [[  0.0000,  62.1779, 211.9324, 145.8790,   0.0000],\n",
      "         [ 19.0214,  70.1495,  70.0391,  98.0498,   0.0000],\n",
      "         [ 68.4448, 143.4875,  82.7936, 182.5480,  14.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n",
      "\n",
      "        [[  0.0000,  65.4863, 224.0000, 160.9617,   0.0000],\n",
      "         [216.6612, 122.4044, 224.0000, 138.3169,   0.0000],\n",
      "         [157.9071, 115.0601, 210.5410, 136.4809,   0.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n",
      "\n",
      "        [[ 72.1778,  60.9111, 224.0000, 205.8889,  18.0000],\n",
      "         [ 87.7333,  52.2000, 207.2000,  99.4889,  18.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n",
      "\n",
      "        [[  0.0000,  57.3440,  77.0907, 215.6373,  14.0000],\n",
      "         [ 93.2187,  58.5387, 224.0000, 189.9520,  14.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n",
      "\n",
      "        [[ 18.2169,   0.0000, 215.2289, 176.0964,  11.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n",
      "\n",
      "        [[120.8567,  21.3970, 224.0000, 197.2537,  18.0000],\n",
      "         [  0.0000,  24.0716, 102.1343, 199.9284,  18.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n",
      "\n",
      "        [[  0.0000,   0.6727, 192.8889, 221.9820,   8.0000],\n",
      "         [ 10.5946,  67.9399, 153.8739, 143.2793,   7.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n",
      "\n",
      "        [[  0.0000,   0.6727, 224.0000, 209.8739,   6.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n",
      "\n",
      "        [[ 34.6453,  26.9147, 173.8240, 224.0000,   2.0000],\n",
      "         [  1.1947,   0.0000, 180.3947, 224.0000,  14.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n",
      "\n",
      "        [[128.3123,   7.3994, 224.0000, 187.6757,   3.0000],\n",
      "         [ 67.7718, 143.9520, 133.0210, 169.5135,   3.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n",
      "\n",
      "        [[  0.4516,  10.3871, 203.6774, 224.0000,   2.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n",
      "\n",
      "        [[ 87.8427, 111.7013,  99.1920, 137.3867,  14.0000],\n",
      "         [107.5547, 108.7147, 126.6693, 154.7093,  14.0000],\n",
      "         [123.6827, 112.2987, 139.2133, 154.7093,  14.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n",
      "         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# `pin_memory` speeds up CPU-GPU batch transfer, `num_workers=NUM_WORKERS` loads data\n",
    "# on the main CPU process, suitable for Colab.\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Use batch_size = 1 during inference - during inference we do not center crop\n",
    "# the image to detect all objects, hence they may be of different size. It is\n",
    "# easier and less redundant to use batch_size=1 rather than zero-padding images.\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "train_loader_iter = iter(train_loader)\n",
    "image_path, images, gt_boxes = train_loader_iter.next()\n",
    "\n",
    "print(f\"image_path has len : {len(image_path)}\")\n",
    "print(f\"images has shape : {images.shape}\")\n",
    "print(f\"gt_boxes has shape    : {gt_boxes.shape}\")\n",
    "\n",
    "print(f\"Five boxes per image  :\")\n",
    "print(gt_boxes[:, :5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "X4WmocEyiXWa",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Visualize PASCAL VOC 2007\n",
    "\n",
    "We will visualize a few images and their GT boxes, just to make sure that everything is loaded properly. You would have already seen these visualizations (and the code snippet below) in `one_stage_detector.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "id": "ld1s28Z4fyL5",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from utils.helper import detection_visualizer\n",
    "\n",
    "inverse_norm = transforms.Compose(\n",
    "    [\n",
    "        transforms.Normalize(mean=[0., 0., 0.], std=[1 / 0.229, 1 / 0.224, 1 / 0.225]),\n",
    "        transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1., 1., 1.]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for idx, (_, image, gt_boxes) in enumerate(train_dataset):\n",
    "    if idx > 2:\n",
    "        break\n",
    "\n",
    "    image = inverse_norm(image)\n",
    "    is_valid = gt_boxes[:, 4] >= 0\n",
    "    detection_visualizer(image, val_dataset.idx_to_class, gt_boxes[is_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWjEwTnwlUFH"
   },
   "source": [
    "# 2 FPN + RPN (5 Points)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UStVHhdCtWip"
   },
   "source": [
    "## 2.1 Backbone with Feature Pyramid Networks (FPN)\n",
    "\n",
    "Faster R-CNN uses a convolutional backbone with FPN in the exact same way as you implemented in FCOS. So you can directly re-use it for this part of the assignment.\n",
    "\n",
    "**NOTE:** Typical state-of-the-art detectors based on Faster R-CNN use four multi-scale features from different FPN levels — `(p2, p3, p4, p5)` with strides `(4, 8, 16, 32)`.\n",
    "Due to computational limits of Google Colab, we will instead simply use `(p3, p4, p5)` features like FCOS.\n",
    "In all your implementations for this part, you may assume that you will receive features from these three FPN levels (and may hard-code these names as Python strings). Your code will not be tested with `p2` FPN features and you will not lose points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8QQpiSZ4lUFI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dummy input images with shape: (2, 3, 224, 224)\n",
      "Shape of c3 features: torch.Size([2, 64, 28, 28])\n",
      "Shape of c4 features: torch.Size([2, 160, 14, 14])\n",
      "Shape of c5 features: torch.Size([2, 400, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from common import DetectorBackboneWithFPN\n",
    "from two_stage_detector import RPNPredictionNetwork\n",
    "\n",
    "\n",
    "backbone = DetectorBackboneWithFPN(out_channels=64)\n",
    "\n",
    "# Pass a batch of dummy images (random tensors) in NCHW format and observe the output.\n",
    "dummy_images = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "# Collect dummy output.\n",
    "dummy_fpn_feats = backbone(dummy_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "oRc7P-RvRZGZ",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 2.2 Faster R-CNN first stage: Region Proposal Network (RPN)\n",
    "\n",
    "We will now implement the first-stage of Faster R-CNN. It comprises a **Region Proposal Network (RPN)** that learns to predict general _object proposals_, which will then be used by the second stage to make final predictions.\n",
    "\n",
    "**RPN prediction:** An input image is passed through the backbone and we obtain its FPN feature maps `(p3, p4, p5)`.\n",
    "The RPN predicts multiple values at _every location on FPN features_. Faster R-CNN is _anchor-based_ — the model assumes that every location has multiple pre-defined boxes (called \"anchors\") and it predicts two measures per anchor, per FPN location:\n",
    "\n",
    "1. **Objectness:** The likelihood of having _any_ object inside the anchor. This is similar to classification head in FCOS, except that this is _class-agnostic_: it only performs binary foreground/background classification.\n",
    "2. **Box regression deltas:** 4-D \"deltas\" that _transform_ an anchor at that location to a ground-truth box.\n",
    "\n",
    "<br>\n",
    "\n",
    "![Network architecture of RPN](images/rpn.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "**SIDE NOTE:** We will use the more common practice of predicting `k` logits and use a logistic regressor instead of `2k` scores (and 2-way softmax) as shown in Figure. This slightly reduces the number of trainable parameters.\n",
    "\n",
    "This RPN is conceptually quite similar to a one-stage detector like FCOS.\n",
    "The main differences with what you implemented in FCOS are: (1) RPN is anchor-based, and make predictions for multiple anchor boxes instead of location \"points\", (2) it performs class-agnostic object classification, and (3) it excludes centerness regression, which was inntroduced in FCOS itself, years after Faster R-CNN was published.\n",
    "\n",
    "Like we saw in FCOS, each anchor will be matched with a GT box for supervision — we will get to it shortly.\n",
    "For now, let's assume there are some `A` anchor boxes at every FPN location, and implement an RPN module.\n",
    "Structurally, this module is similar to FCOS prediction network.\n",
    "Now follow the instructions in `RPNPredictionNetwork` of `two_stage_detector.py` and implement layers to predict objectness and box regression deltas.\n",
    "Execute the following cell to test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JZ17IcMTlUFJ"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m rpn_pred_net \u001b[38;5;241m=\u001b[39m RPNPredictionNetwork(\n\u001b[0;32m      4\u001b[0m     in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, stem_channels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m], num_anchors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Pass the dummy FPN feats to RPN prediction network and view its output shapes.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m dummy_rpn_obj, dummy_rpn_box \u001b[38;5;241m=\u001b[39m \u001b[43mrpn_pred_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_fpn_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Few expected outputs:\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Shape of p4 RPN objectness: torch.Size([2, 196, 3])\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Shape of p5 RPN box deltas: torch.Size([2, 49, 12])\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFor dummy input images with shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdummy_images\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\PycharmProjects\\UniProjects\\CV\\assignment5\\two_stage_detector.py:156\u001b[0m, in \u001b[0;36mRPNPredictionNetwork.forward\u001b[1;34m(self, feats_per_fpn_level)\u001b[0m\n\u001b[0;32m    153\u001b[0m     boxreg_deltas[key] \u001b[38;5;241m=\u001b[39m x_box\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Make some checks\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m _B, _C, _H, _W \u001b[38;5;241m=\u001b[39m \u001b[43mfeats_per_fpn_level\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m object_logits \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m object_logits \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m object_logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing keys in object_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m boxreg_deltas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m boxreg_deltas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m boxreg_deltas, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing keys in boxreg_deltas\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from two_stage_detector import RPNPredictionNetwork\n",
    "\n",
    "rpn_pred_net = RPNPredictionNetwork(\n",
    "    in_channels=64, stem_channels=[64], num_anchors=3\n",
    ")\n",
    "\n",
    "# Pass the dummy FPN feats to RPN prediction network and view its output shapes.\n",
    "dummy_rpn_obj, dummy_rpn_box = rpn_pred_net(dummy_fpn_feats)\n",
    "\n",
    "# Few expected outputs:\n",
    "# Shape of p4 RPN objectness: torch.Size([2, 196, 3])\n",
    "# Shape of p5 RPN box deltas: torch.Size([2, 49, 12])\n",
    "\n",
    "print(f\"\\nFor dummy input images with shape: {dummy_images.shape}\")\n",
    "for level_name in dummy_fpn_feats.keys():\n",
    "    print(f\"Shape of {level_name} FPN features  : {dummy_fpn_feats[level_name].shape}\")\n",
    "    print(f\"Shape of {level_name} RPN objectness: {dummy_rpn_obj[level_name].shape}\")\n",
    "    print(f\"Shape of {level_name} RPN box deltas: {dummy_rpn_box[level_name].shape}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "etBYc7rbj35F",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "# 3 Training Target RPN (25 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6K-cxgUtjWl"
   },
   "source": [
    "\n",
    "## 3.1 Anchor-based Training of RPN\n",
    "\n",
    "Now that we implemented this RPN head, our goal is to have it predict _high objectness_ and _accurate box deltas_ for anchors that are likely to contain objects.\n",
    "Similar to first part of our assignment, we need to assign a target GT box to every RPN prediction for training supervision.\n",
    "\n",
    "**Recall FCOS location matching:** FCOS matched every FPN feature map location with a GT box (or marked them background), based on a heuristic that a location whether that location was _inside_ any GT Box.\n",
    "On the other hand, Faster R-CNN is anchor-based: instead of _locations_, it makes predictions with reference to some pre-defined _anchor boxes_, and matches each anchor with a single GT box if they have a high enough Intersection-over-Union (IoU).\n",
    "\n",
    "In the next few cells, we will perform the following steps, which are procedurally very similar to what you have already done with FCOS:\n",
    "\n",
    "1. **Anchor generation:** Generate a set of anchors for every location in FPN feature map.\n",
    "2. **Anchor to GT matching:** Match these anchors with GT boxes based on their IoU-overlap.\n",
    "3. **Format of box deltas:** Implement the tranformation functions to obtain _box deltas_ from GT boxes (model training supervision) and apply deltas to anchors (final proposal boxes for second stage).\n",
    "\n",
    "Let's approach these steps, one at a time.\n",
    "\n",
    "1.   List item\n",
    "2.   List item\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJZ1YT8ztlCg"
   },
   "source": [
    "### 3.1.1 Anchor Generation\n",
    "\n",
    "Recall that you already implemented a function to get the absolute image co-ordinates of FPN feature map locations, for FCOS — in `get_fpn_location_coords` of `common.py`.\n",
    "First we need to form multiple anchor boxes centered at these locations.\n",
    "RPN defines square anchor boxes of size `scale * stride` at every location, where `stride` is the FPN level stride, and `scale` is a hyperparameter.\n",
    "For example, anchor boxes for P5 level (`stride = 32`), with `scale = 2` will be boxes of `(64 x 64)` pixels.\n",
    "RPN also considers anchors of different aspect ratios, apart from square anchor boxes —\n",
    "follow the instructions in `generate_fpn_anchors` of `two_stage_detector.py` to implement creation of multiple anchor boxes per location.\n",
    "\n",
    "Execute the next cell to verify your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "id": "O5w-EUJekJj-",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from common import get_fpn_location_coords\n",
    "from two_stage_detector import generate_fpn_anchors\n",
    "\n",
    "\n",
    "# Sanity check: Get 2x2 location co-ordinates of p5 (original shape is 7x7).\n",
    "locations = get_fpn_location_coords(\n",
    "    shape_per_fpn_level={\"p5\": (2, 64, 2, 2)}, strides_per_fpn_level={\"p5\": 32}\n",
    ")\n",
    "\n",
    "print(\"P5 locations:\\n\", locations[\"p5\"])\n",
    "\n",
    "# Generate anchors for these locations.\n",
    "anchors = generate_fpn_anchors(\n",
    "    locations_per_fpn_level=locations,\n",
    "    strides_per_fpn_level={\"p5\": 32},\n",
    "    stride_scale=2,\n",
    "    aspect_ratios=[0.5, 1.0, 2.0],\n",
    ")\n",
    "\n",
    "print(\"P5 anchors with different aspect ratios:\")\n",
    "print(\"P5 1:2 anchors:\\n\", anchors[\"p5\"][0::3, :], \"\\n\")\n",
    "# Expected (any ordering is fine):\n",
    "# [-29.2548,  -6.6274,  61.2548,  38.6274]\n",
    "# [-29.2548,  25.3726,  61.2548,  70.6274]\n",
    "# [  2.7452,  -6.6274,  93.2548,  38.6274]\n",
    "# [  2.7452,  25.3726,  93.2548,  70.6274]\n",
    "\n",
    "print(\"P5 1:1 anchors:\\n\", anchors[\"p5\"][1::3, :], \"\\n\")\n",
    "# Expected (any ordering is fine):\n",
    "# [-16., -16.,  48.,  48.]\n",
    "# [-16.,  16.,  48.,  80.]\n",
    "# [ 16., -16.,  80.,  48.]\n",
    "# [ 16.,  16.,  80.,  80.]\n",
    "\n",
    "print(\"P5 2:1 anchors:\\n\", anchors[\"p5\"][2::3, :], \"\\n\")\n",
    "# Similar to 1:2 anchors, but with flipped co-ordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gd_w0_qSlUFK"
   },
   "source": [
    "### 3.1.2 Matching anchor boxes with GT boxes\n",
    "\n",
    "Similar to FCOS, we will now match these generated anchors with GT boxes. Faster R-CNN matches some `N` anchor boxes with `M` GT boxes by applying a simple rule:\n",
    "\n",
    "> Anchor box $N_i$ is matched with box $M_i$ if it has an IoU overlap higher than 0.6 with that box. For multiple such GT boxes, the anchor is assigned with the GT box that has the highest IoU. Note that a single ground-truth box may assign positive labels to multiple anchors.\n",
    "\n",
    "**NOTE:** _Faster R-CNN uses 0.7 default threshold_ as mentioned in the lecture slides. We use a lower threeshold to increase the number of positive matches for sampling — this helps in speeding up training in a resource constrained setting like Google Colab.\n",
    "\n",
    "Anchor boxes with `IoU < 0.3` with ALL GT boxes is assigned background GT box `(-1, -1, -1, -1, -1)`. All other anchors with IoU between `(0.3, 0.6)` are considered \"neutral\" and ignored during training. This matching differs from FCOS, which assigns ALL anchors to either object or background — the \"neutral\" Faster R-CNN anchors cause wasted computation, and removing this redundancy would overly complicate our implementation.\n",
    "\n",
    "We have implemented this matching procedure for you — see `rcnn_match_anchors_to_gt` of `two_stage_detector.py`.\n",
    "Read its documentation to understand its input/output format, it is slightly different than `fcos_match_locations_to_gt`.\n",
    "It serves the same purpose as location matching in FCOS — to define GT targets for model predictions during training.\n",
    "\n",
    "This function internally requires IoU computation between all anchors and GT boxes — which you have to implement.\n",
    "Follow the instructions in `two_stage_detector.py` to implement IoU computation, and execute the next cell for a sanity check — you should observe an error of `1e-7` or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "button": false,
    "collapsed": false,
    "id": "fK_USCuaXSzh",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOXES 1 SHAPE AFTER tensor([[10., 10., 90., 90.],\n",
      "        [10., 10., 90., 90.],\n",
      "        [10., 10., 90., 90.],\n",
      "        [20., 20., 40., 40.],\n",
      "        [20., 20., 40., 40.],\n",
      "        [20., 20., 40., 40.],\n",
      "        [60., 60., 80., 80.],\n",
      "        [60., 60., 80., 80.],\n",
      "        [60., 60., 80., 80.]])\n",
      "BOXES 2 SHAPE AFTER tensor([[10., 10., 90., 90.],\n",
      "        [60., 60., 80., 80.],\n",
      "        [30., 30., 70., 70.],\n",
      "        [10., 10., 90., 90.],\n",
      "        [60., 60., 80., 80.],\n",
      "        [30., 30., 70., 70.],\n",
      "        [10., 10., 90., 90.],\n",
      "        [60., 60., 80., 80.],\n",
      "        [30., 30., 70., 70.]])\n",
      "intersection tensor([6400.,  400., 1600.,  400.,    0.,  100.,  400.,  400.,  100.])\n",
      "area1 tensor([6400., 6400., 6400.,  400.,  400.,  400.,  400.,  400.,  400.])\n",
      "area2 tensor([6400.,  400., 1600., 6400.,  400., 1600., 6400.,  400., 1600.])\n",
      "UNION SHAPE torch.Size([9])\n",
      "tensor([[1.0000, 0.0625, 0.2500],\n",
      "        [0.0625, 0.0000, 0.0526],\n",
      "        [0.0625, 1.0000, 0.0526]])\n",
      "Relative error: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from two_stage_detector import iou\n",
    "\n",
    "\n",
    "boxes1 = torch.Tensor([[10, 10, 90, 90], [20, 20, 40, 40], [60, 60, 80, 80]])\n",
    "boxes2 = torch.Tensor([[10, 10, 90, 90], [60, 60, 80, 80], [30, 30, 70, 70]])\n",
    "\n",
    "expected_iou = torch.Tensor(\n",
    "    [[1.0, 0.0625, 0.25], [0.0625, 0.0, 0.052631579], [0.0625, 1.0, 0.052631579]]\n",
    ")\n",
    "result_iou = iou(boxes1, boxes2)\n",
    "print(result_iou)\n",
    "\n",
    "print(\"Relative error:\", rel_error(expected_iou, result_iou))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPMAI3WFlUFL"
   },
   "source": [
    "### Visualizing matched GT boxes\n",
    "\n",
    "Now we apply our anchor matching function and visualize one GT box with a random matched positive anchor box.\n",
    "You may try different images by indexing `train_dataset` below,\n",
    "make sure to try different FPN levels as certain images may not get any matched GT boxes due to their size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "id": "wPvX4TrgaLD8",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from common import get_fpn_location_coords\n",
    "from two_stage_detector import generate_fpn_anchors, rcnn_match_anchors_to_gt\n",
    "\n",
    "\n",
    "# Sanity check: Match anchors of p4 level with GT boxes of first image\n",
    "# in the training dataset.\n",
    "_, image, gt_boxes = train_dataset[0]\n",
    "\n",
    "FPN_LEVEL = \"p4\"\n",
    "FPN_STRIDE = 16\n",
    "locations = get_fpn_location_coords(\n",
    "    shape_per_fpn_level={FPN_LEVEL: (2, 64, 224 // FPN_STRIDE, 224 // FPN_STRIDE)},\n",
    "    strides_per_fpn_level={FPN_LEVEL: FPN_STRIDE}\n",
    ")\n",
    "# Generate anchors for these locations.\n",
    "anchors = generate_fpn_anchors(\n",
    "    locations_per_fpn_level=locations,\n",
    "    strides_per_fpn_level={FPN_LEVEL: FPN_STRIDE},\n",
    "    stride_scale=8,  # Default value used by Faster R-CNN\n",
    "    aspect_ratios=[0.5, 1.0, 2.0],\n",
    ")\n",
    "\n",
    "matched_gt_boxes = rcnn_match_anchors_to_gt(\n",
    "    anchors[FPN_LEVEL], gt_boxes, iou_thresholds=(0.3, 0.6)\n",
    ")\n",
    "\n",
    "# Flatten anchors and matched boxes:\n",
    "anchors_p4 = anchors[FPN_LEVEL].view(-1, 4)\n",
    "matched_boxes_p4 = matched_gt_boxes.view(-1, 5)\n",
    "\n",
    "# Visualize one selected anchor and its matched GT box.\n",
    "# NOTE: Run this cell multiple times to see different matched anchors. For car\n",
    "# image, p3/5 will not work because the GT box was already assigned to p4.\n",
    "fg_idxs_p4 = (matched_boxes_p4[:, 4] > 0).nonzero()\n",
    "fg_idx = random.choice(fg_idxs_p4)\n",
    "\n",
    "# Combine both boxes for visualization:\n",
    "dummy_vis_boxes = [anchors_p4[fg_idx][0], matched_boxes_p4[fg_idx][0]]\n",
    "\n",
    "print(\"Unlabeled red box is positive anchor:\")\n",
    "detection_visualizer(\n",
    "    inverse_norm(image),\n",
    "    val_dataset.idx_to_class,\n",
    "    bbox=dummy_vis_boxes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "XW_Zek3_dgfF",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### 3.1.3 GT Targets for box regression\n",
    "\n",
    "Now we work on the third and final component needed to train our RPN — we define transformation functions for box regression deltas.\n",
    "Recall in the first part of the assignment, you implemented two such functions for FCOS (quoting from `one_stage_detector.ipynb`):\n",
    "\n",
    "> 1. `fcos_get_deltas_from_locations`: Accepts locations (centers) and GT boxes, and returns deltas. Required for training supervision.\n",
    "> 2. `fcos_apply_deltas_to_locations`: Accepts predicted deltas and locations, and returns predicted boxes. Required during inference.\n",
    "\n",
    "Here you will implement similar transformation functions for R-CNN, albeit with a different transformation logic than FCOS. We refer to the lecture of University of Michigan [Lecture 13, slides 68-71](https://web.eecs.umich.edu/~justincj/slides/eecs498/WI2022/598_WI2022_lecture13.pdf), follow these and implement two functions in `two_stage_detector.py`:\n",
    "\n",
    "1. `rcnn_get_deltas_from_anchors`: Accepts anchor boxes and GT boxes, and returns deltas. Required for training supervision.\n",
    "2. `rcnn_apply_deltas_to_anchors`: Accepts predicted deltas and anchor boxes, and returns predicted boxes. Required during inference.\n",
    "\n",
    "Run the following cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "id": "MX2JCaOf0768",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from two_stage_detector import rcnn_get_deltas_from_anchors, rcnn_apply_deltas_to_anchors\n",
    "\n",
    "# Three hard-coded anchor boxes and GT boxes that have a fairly high overlap.\n",
    "# Add a dummy class ID = 1 indicating foreground\n",
    "input_anchors = torch.Tensor(\n",
    "    [[20, 40, 80, 90], [10, 10, 50, 50], [120, 100, 200, 200]]\n",
    ")\n",
    "input_boxes = torch.Tensor(\n",
    "    [[10, 15, 100, 115, 1], [30, 20, 40, 30, 1], [120, 100, 200, 200, 1]]\n",
    ")\n",
    "\n",
    "# Here we do a simple sanity check - getting deltas for a particular set of boxes\n",
    "# and applying them back to anchors should give us the same boxes.\n",
    "_deltas = rcnn_get_deltas_from_anchors(input_anchors, input_boxes)\n",
    "output_boxes = rcnn_apply_deltas_to_anchors(_deltas, input_anchors)\n",
    "\n",
    "print(\"Rel error in reconstructed boxes:\", rel_error(input_boxes[:, :4], output_boxes))\n",
    "\n",
    "# Another check: deltas for GT class label = -1 should be -1e8\n",
    "background_box = torch.Tensor([[-1, -1, -1, -1, -1]])\n",
    "input_anchor = torch.Tensor([[100, 100, 200, 200]])\n",
    "\n",
    "_deltas = rcnn_get_deltas_from_anchors(input_anchor, background_box)\n",
    "output_box = rcnn_apply_deltas_to_anchors(_deltas, input_anchor)\n",
    "\n",
    "print(\"Background deltas should be all -1e8  :\", _deltas)\n",
    "print(\"Output box should be -1e8 or lower    :\", output_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "dlO2IUCnt4zu",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 3.2 Loss Functions\n",
    "\n",
    "With all predictions assigned with GT targets, we will proceed to compute losses for training the RPN.\n",
    "Recall that you used [Focal Loss](https://arxiv.org/abs/1708.02002) for classification and L1 loss for box regression in FCOS.\n",
    "Here, you will use L1 loss for box regression, similar to FCOS.\n",
    "\n",
    "**Objectness classification loss:** Focal Loss was proposed in RetinaNet (2017) to deal with heavy class imbalance caused by \"background\". Faster R-CNN predates this paper — it dealt with class imbalance by randomly sampling roughly equal amount of foreground-background anchors to train RPN. We have implemented a very simple sampling function for you in `sample_rpn_training` function of `two_stage_detector.py` — you may directly use it while you piece all these components (coming up next).\n",
    "\n",
    "**Total loss** is the sum of both loss components _per sampled anchor_, averaged by total number of foreground + background anchors.\n",
    "\n",
    "Execute the next cell to quickly recap their usage — you have already seen these in the first part of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "id": "2eSleGX9yTeo",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# Sanity check: dummy predictions from model - box regression deltas and\n",
    "# objectness logits for two anchors.\n",
    "# shape: (batch_size, HWA, 4 or 1)\n",
    "dummy_pred_boxreg_deltas = torch.randn(1, 2, 4)\n",
    "dummy_pred_obj_logits = torch.randn(1, 2, 1)\n",
    "\n",
    "# Dummy deltas and objectness targets. Let the second box be background.\n",
    "# Dummy GT boxes (matched with both anchors).\n",
    "dummy_gt_deltas = torch.randn_like(dummy_pred_boxreg_deltas)\n",
    "dummy_gt_deltas[:, 1, :] = -1e8\n",
    "\n",
    "# Background objectness targets should be 0 (not -1), and foreground\n",
    "# should be 1. Neutral anchors will not occur here due to sampling.\n",
    "dummy_gt_objectness = torch.Tensor([1, 0])\n",
    "\n",
    "# Note that loss is not multiplied with 0.25 here:\n",
    "loss_box = F.l1_loss(\n",
    "    dummy_pred_boxreg_deltas, dummy_gt_deltas, reduction=\"none\"\n",
    ")\n",
    "\n",
    "# No loss for background anchors:\n",
    "loss_box[dummy_gt_deltas == -1e8] *= 0.0\n",
    "print(\"Box regression loss (L1):\", loss_box)\n",
    "\n",
    "# Now calculate objectness loss.\n",
    "loss_obj = F.binary_cross_entropy_with_logits(\n",
    "    dummy_pred_obj_logits.view(-1), dummy_gt_objectness, reduction=\"none\"\n",
    ")\n",
    "print(\"Objectness loss (BCE):\", loss_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "AiPfXUHPupDE",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 3.3 Putting it all together: RPN module\n",
    "\n",
    "Now you will put together all the things you have implemented into the `RPN` class in `two_stage_detector.py`.\n",
    "Implement `forward` and `predict_proposals` functions of this module — you have already done most of the heavy lifting, you simply need to call all the functions in a correct way!\n",
    "Use the previous two cells as a reference to implement loss calculation in `forward()`.\n",
    "\n",
    "**TIP:** It may help if you draw analogies between the implementation logic in this module vs FCOS (`RPN.predict_proposals()` -> `FCOS.inference()`).\n",
    "\n",
    "## Overfit small data\n",
    "\n",
    "In Faster R-CNN, the RPN is trained jointly with the second-stage network.\n",
    "However, to test our RPN implementation, we will first train just the RPN — this is basically a class-agnostic FCOS without centerness.\n",
    "We will use the `train_detector` function that we used for training FCOS.\n",
    "You can read its implementation in `utils/helper.py`. \n",
    "\n",
    "The loss should generally do down, however the forward pass here is a bit slower than FCOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "id": "YTObddiog9wJ",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from utils.helper import train_detector\n",
    "from common import DetectorBackboneWithFPN\n",
    "from two_stage_detector import RPN\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# Take equally spaced examples from training dataset to make a subset.\n",
    "small_dataset = torch.utils.data.Subset(\n",
    "    train_dataset,\n",
    "    torch.linspace(0, len(train_dataset) - 1, steps=BATCH_SIZE * 10).long()\n",
    ")\n",
    "small_train_loader = torch.utils.data.DataLoader(\n",
    "    small_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Create a wrapper module to contain backbone + RPN:\n",
    "class FirstStage(nn.Module):\n",
    "    def __init__(self, fpn_channels: int):\n",
    "        super().__init__()\n",
    "        self.backbone = DetectorBackboneWithFPN(out_channels=fpn_channels)\n",
    "        self.rpn = RPN(\n",
    "            fpn_channels=fpn_channels,\n",
    "            # Simple stem of two layers:\n",
    "            stem_channels=[fpn_channels, fpn_channels],\n",
    "            batch_size_per_image=16,\n",
    "            anchor_stride_scale=8,\n",
    "            anchor_aspect_ratios=[0.5, 1.0, 2.0],\n",
    "            anchor_iou_thresholds=(0.3, 0.6),\n",
    "        )\n",
    "\n",
    "    def forward(self, images, gt_boxes=None):\n",
    "        feats_per_fpn_level = self.backbone(images)\n",
    "        return self.rpn(feats_per_fpn_level, self.backbone.fpn_strides, gt_boxes)\n",
    "\n",
    "\n",
    "first_stage = FirstStage(fpn_channels=64).to(DEVICE)\n",
    "\n",
    "train_detector(\n",
    "    first_stage,\n",
    "    small_train_loader,\n",
    "    learning_rate=8e-3,\n",
    "    max_iters=1000,\n",
    "    log_period=20,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "jKjv6JLMRj7s",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "# 4 Faster R-CNN (20 Points)\n",
    "---\n",
    "\n",
    "We have implemented the first half of Faster R-CNN, i.e., RPN, which is class-agnostic. Here, we briefly describe the second half Fast R-CNN.\n",
    "\n",
    "Given a set of proposal boxes from RPN (per FPN level, per image),\n",
    "we warp each region from the correspondng map to a fixed size 7x7 by using [RoI Align](https://arxiv.org/pdf/1703.06870.pdf).\n",
    "We will use the `roi_align` function from `torchvision`. For usage instructions, see https://pytorch.org/docs/stable/torchvision/ops.html#torchvision.ops.roi_align\n",
    "\n",
    "For simplicity and computational constraints of Google Colab,\n",
    "our two-stage detector here differs from a standard Faster R-CNN system in the second stage:\n",
    "In a full implementation, the second stage of the network would predict a box deltas to further refine RPN proposals.\n",
    "We omit this for simplicity and keep RPN proposal boxes as final predictions.\n",
    "Your model will definitely perform better if you add an extra box regression head in second stage.\n",
    "\n",
    "### Your implementation exercise\n",
    "\n",
    "Read `FasterRCNN` class documentation and code to understand how everything is pieced together.\n",
    "By now you have already implemented the core components of a typical object detection system - you have dealt with anchor boxes or locations (FCOS), matched them with GT boxes, supervised model with your matching, and wrote inference utilities like NMS.\n",
    "Great work!\n",
    "\n",
    "### Classification Loss: cross entropy\n",
    "\n",
    "The classification loss for second-stage is a cross entropy loss — you would have seen this in a previous exercise, and it is a multi-class extension of binary cross entropy loss used in RPN objectness classification. You may use `torch.nn.functional.cross_entropy` directly — follow instructions in Python script.\n",
    "\n",
    "Beyond these, the second stage of Faster R-CNN doesn't add anything that is conceptually new — hence your implementation exercise is fairly lightweight.\n",
    "We have implemented most of this module for you. We left out a few 3-4 line TODO blocks, only because if we wrote them, they would given away the solution for prior exercises (RPN and FCOS).\n",
    "Moreover, empty code blocks will encourage you to carefully read the remaining portions for making everything work.\n",
    "Feel free to refer/re-use your own implementation from the first part of the assignment for filling these blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "RFZ49wox4MYn",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Overfit small data\n",
    "\n",
    "After adding your implementation, overfit the model on a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "id": "WbxeAJq0zc3F",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from two_stage_detector import FasterRCNN\n",
    "\n",
    "\n",
    "# Re-initialize dataset objects for independent debugging.\n",
    "small_dataset = torch.utils.data.Subset(\n",
    "    train_dataset,\n",
    "    torch.linspace(0, len(train_dataset) - 1, steps=BATCH_SIZE * 10).long()\n",
    ")\n",
    "small_train_loader = torch.utils.data.DataLoader(\n",
    "    small_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "FPN_CHANNELS = 64\n",
    "backbone = DetectorBackboneWithFPN(out_channels=FPN_CHANNELS)\n",
    "rpn = RPN(\n",
    "    fpn_channels=FPN_CHANNELS,\n",
    "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
    "    batch_size_per_image=16,\n",
    "    anchor_stride_scale=8,\n",
    "    anchor_aspect_ratios=[0.5, 1.0, 2.0],\n",
    "    anchor_iou_thresholds=(0.3, 0.6),\n",
    "    pre_nms_topk=400,\n",
    "    post_nms_topk=80,\n",
    ")\n",
    "\n",
    "faster_rcnn = FasterRCNN(\n",
    "    backbone, rpn, num_classes=20, roi_size=(7, 7),\n",
    "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
    "    batch_size_per_image=32,\n",
    ")\n",
    "\n",
    "train_detector(\n",
    "    faster_rcnn,\n",
    "    small_train_loader,\n",
    "    learning_rate=0.01,\n",
    "    max_iters=1000,\n",
    "    log_period=10,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "_SWA1DbG47ln",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Inference\n",
    "\n",
    "Now, follow the instructions in `FasterRCNN.inference` to implement inference, similar to `FCOS.inference`.\n",
    "\n",
    "Visualize the output from the trained model on a few images by executing the next cell, the bounding boxes should be somewhat accurate. They would get even better by using a bigger model and training it for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "collapsed": false,
    "id": "gp_Hmt-Km5bl",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "b2fe0c8e-fb0e-4151-8021-ca7637c95b2a",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from utils.helper import inference_with_detector\n",
    "\n",
    "\n",
    "# Change the loader to have (batch size = 1) as required for inference.\n",
    "small_train_loader = torch.utils.data.DataLoader(\n",
    "    small_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "inference_with_detector(\n",
    "    faster_rcnn,\n",
    "    small_train_loader,\n",
    "    val_dataset.idx_to_class,\n",
    "    score_thresh=0.2,\n",
    "    nms_thresh=0.5,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "sr7wNngy4oZf",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Train a net\n",
    "\n",
    "Now it's time to train the full Faster R-CNN model on a larger subset of the the training data.\n",
    "We will train for 9000 iterations; this should take about 2-3 hours on a K80 GPU.\n",
    "Note that real object detection systems typically train for 12-24 hours, distribute training over multiple GPUs, and use much faster GPUs. As such our result will be far from the state of the art, but it should give some reasonable results!\n",
    "\n",
    "(Optional) If you train the model longer (e.g., 25K+ iterations), you should see a better mAP. But make sure you revert the code back for grading purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "id": "X1k1rx1f4sTE",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "reset_seed(0)\n",
    "\n",
    "# Slightly larger detector than in above cell.\n",
    "FPN_CHANNELS = 128\n",
    "backbone = DetectorBackboneWithFPN(out_channels=FPN_CHANNELS)\n",
    "rpn = RPN(\n",
    "    fpn_channels=FPN_CHANNELS,\n",
    "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
    "    batch_size_per_image=16,\n",
    "    pre_nms_topk=500,\n",
    "    post_nms_topk=200  # Other args from previous cell are default args in RPN.\n",
    ")\n",
    "\n",
    "faster_rcnn = FasterRCNN(\n",
    "    backbone, rpn, num_classes=NUM_CLASSES, roi_size=(7, 7),\n",
    "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
    "    batch_size_per_image=32,\n",
    ")\n",
    "\n",
    "\n",
    "train_detector(\n",
    "    faster_rcnn,\n",
    "    train_loader,\n",
    "    learning_rate=0.01,\n",
    "    max_iters=9000,\n",
    "    log_period=50,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "# After you've trained your model, save the weights for submission.\n",
    "weights_path = ROOT_PATH /   \"rcnn_detector.pt\"\n",
    "torch.save(faster_rcnn.state_dict(), weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "KhWZT-ztEaqm",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Inference\n",
    "\n",
    "VIsualize a few outputs from the full trained model. They may be less accurate than FCOS.\n",
    "This is expected since our Faster R-CNN model is weaker than expected: we used a smaller model, trained for short duration, and did not include box regression in the second stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": false,
    "id": "J7ArGiLTnHta",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "ba366134-e564-40db-c4ec-f6b18c0abb08",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Add some imports to run this cell independently of above few cells\n",
    "# (you will need to run first few cells at the top)\n",
    "from utils.helper import inference_with_detector\n",
    "from common import DetectorBackboneWithFPN\n",
    "from two_stage_detector import RPN, FasterRCNN\n",
    "\n",
    "\n",
    "# Re-initialize so this cell is independent from prior cells.\n",
    "# Slightly larger detector than in above cell.\n",
    "FPN_CHANNELS = 128\n",
    "backbone = DetectorBackboneWithFPN(out_channels=FPN_CHANNELS)\n",
    "rpn = RPN(fpn_channels=FPN_CHANNELS, stem_channels=[FPN_CHANNELS, FPN_CHANNELS])\n",
    "faster_rcnn = FasterRCNN(\n",
    "    backbone, rpn, num_classes=NUM_CLASSES, roi_size=(7, 7),\n",
    "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
    "    batch_size_per_image=32,\n",
    ")\n",
    "faster_rcnn.to(device=DEVICE)\n",
    "\n",
    "weights_path = ROOT_PATH / \"rcnn_detector.pt\"\n",
    "faster_rcnn.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\n",
    "\n",
    "# Prepare a small val daataset for inference:\n",
    "small_dataset = torch.utils.data.Subset(\n",
    "    val_dataset,\n",
    "    torch.linspace(0, len(val_dataset) - 1, steps=20).long()\n",
    ")\n",
    "small_val_loader = torch.utils.data.DataLoader(\n",
    "    small_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "inference_with_detector(\n",
    "    faster_rcnn,\n",
    "    small_val_loader,\n",
    "    val_dataset.idx_to_class,\n",
    "    score_thresh=0.2,\n",
    "    nms_thresh=0.5,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "ETU6ev7aydIY",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate your Faster R-CNN like FCOS.\n",
    "(**NOTE:** It is okay if your model does not perform as good as your FCOS implementation, since we didn't train for FCOS.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "id": "FvDb7uwqyhAK",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "inference_with_detector(\n",
    "    faster_rcnn,\n",
    "    val_loader,\n",
    "    val_dataset.idx_to_class,\n",
    "    score_thresh=0.2,\n",
    "    nms_thresh=0.5,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float32,\n",
    "    output_dir=\"mAP/input\",\n",
    ")\n",
    "!cd mAP && python main.py\n",
    "\n",
    "# This script outputs an image containing per-class AP. Display it here:\n",
    "from IPython.display import Image\n",
    "Image(filename=\"./mAP/output/mAP.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQDMRoxxlUFS"
   },
   "source": [
    "<br>\n",
    "\n",
    "### Acknowledgement\n",
    "\n",
    "This exercise is adapted from the [EECS 498-007/598-005 University of Michigan](https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/) course."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RFZ49wox4MYn"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "interpreter": {
   "hash": "4ef42baa288ce895b984811292da1481faa2138d6a325169bc8d9d38d49f8a2b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
