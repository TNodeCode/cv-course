{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "**Goethe University Frankfurt am Main**\n",
    "\n",
    "Winter Semester 2022/23\n",
    "\n",
    "<br>\n",
    "\n",
    "## *Assignment 8 (Multimodal Learning)*\n",
    "\n",
    "---\n",
    "\n",
    "**Points:** 40<br>\n",
    "**Due:** 9.2.2022, 10 am<br>\n",
    "**Contact:** Timothy Schauml√∂ffel (Discord)<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Your Name:** Tilo-Lars Flasche\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multimodal Fusion\n",
    "\n",
    "---\n",
    "In this task we want to investigate to what extent different modalities can help in solving a problem. For this we will use the [MuMu](https://arxiv.org/pdf/1707.04916.pdf) dataset, which specifies the **genre of music albums**. We focus on the two modalities audio and vision. To reduce complexity, these are provided as vectors already embedded by a unimodal encoder.\n",
    "\n",
    "For the audio data, a CNN was trained on the spectrograms of the individual tracks and the average was taken for the whole album. The visual part of an album corresponds to the cover image. It was embedded using a ResNet101.\n",
    "\n",
    "A single album can have multiple genres, so this becomes a multi-label classification problem. Moreover, these genres are very unbalanced, as you can see in the figure below. The number of classes has been reduced to 20 for this assignment.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"assets/top20_genre.png\" width=1024 height=400/></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "We trained a simple classification head on top of the uni-modal features. Since it's a multi-label problem we used binary cross entropy loss and evaluate with mean average precision. The results are as follows:\n",
    "\n",
    "$$\n",
    "    \\text{mAP}_{audio} = 0.560 \\\\\n",
    "    \\text{mAP}_{vision} = 0.292\n",
    "$$\n",
    "\n",
    "During the exercise we will try to improve these results by combining modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from multimodal.solver import Solver, show_training\n",
    "from multimodal.dataset import AlbumGenreDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the prepared dataset from [here](https://www.dropbox.com/s/iz54mea432vg8o1/album_genre_dataset.zip?dl=0) and decompress to the project directory (or to any location but change the `root` value in the follownig cell).\n",
    "\n",
    "The dataset is loaded by the `AlbumGenreDataset` class. It returns a tuple `(audio_features, visual_features)` of normalized features and the one-hot encoded ground-truth genres. If you run the following cell you can the output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'album_genre_dataset'\n",
    "num_classes = 20\n",
    "num_epochs = 10\n",
    "\n",
    "train_ds = AlbumGenreDataset(root, 'train')\n",
    "val_ds = AlbumGenreDataset(root, 'val')\n",
    "\n",
    "x, y = train_ds[0]\n",
    "print(f\"Audio features shape : {x[0].shape}\")\n",
    "print(f\"Visual features shape: {x[1].shape}\")\n",
    "print(f\"Target shape         : {y.shape}\")\n",
    "print(f\"Target: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1: Fusion (8 Points)\n",
    "\n",
    "Create a simple model that expects visual and auditory embeddings, concatenates them and forwards them through a MLP for the final prediction. Use the provided `Solver` to train the model. The training loop is already implemented and configured for you. The loss function is `BCEWithLogitsLoss`, so you don't have to apply any activation one the prediction. For evaluation, we use the `torchmetrics` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "##                   START OF YOUR CODE                   ##\n",
    "############################################################\n",
    "\n",
    "class GiveMeAName(nn.Module):\n",
    "    pass\n",
    "\n",
    "############################################################\n",
    "##                   END OF YOUR CODE                     ##\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None  # initialize your module\n",
    "\n",
    "solver = Solver(\n",
    "    model=model,\n",
    "    data={\n",
    "        'train': train_ds,\n",
    "        'val': val_ds\n",
    "    },\n",
    "    loss='BCEWithLogitsLoss',\n",
    "    loss_config={},\n",
    "    optimizer='Adam',\n",
    "    optimizer_config={'lr': 1e-4},\n",
    "    batch_size=32,\n",
    "    metric='AveragePrecision', # see torchmetrics.AveragePrecision for details\n",
    "    metric_config={'num_labels': num_classes, 'task': 'multilabel'},\n",
    ")\n",
    "\n",
    "hist = solver.train(num_epochs=num_epochs)\n",
    "print(f\"Best mAP: {max(hist['val_acc']):.3f}\")\n",
    "show_training(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: Gated Fusion (12 Points)\n",
    "\n",
    "Implement a model that fused both modalities using a [Gated Multimodal Unit](https://arxiv.org/pdf/1702.01992.pdf). After fusion, forward the resul through a simple MLP for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "##                   START OF YOUR CODE                   ##\n",
    "############################################################\n",
    "\n",
    "class GiveMeAName2(nn.Module):\n",
    "    pass\n",
    "\n",
    "############################################################\n",
    "##                   END OF YOUR CODE                     ##\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None  # initialize your model\n",
    "\n",
    "solver = Solver(\n",
    "    model=model,\n",
    "    data={\n",
    "        'train': train_ds,\n",
    "        'val': val_ds\n",
    "    },\n",
    "    loss='BCEWithLogitsLoss',\n",
    "    loss_config={},\n",
    "    optimizer='Adam',\n",
    "    optimizer_config={'lr': 1e-4},\n",
    "    batch_size=32,\n",
    "    metric='AveragePrecision',\n",
    "    metric_config={'num_labels': num_classes, 'task': 'multilabel'},\n",
    ")\n",
    "\n",
    "hist = solver.train(num_epochs=num_epochs)\n",
    "print(f\"Best mAP: {max(hist['val_acc']):.3f}\")\n",
    "show_training(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3 (Optional)\n",
    "Can you get even better? Feel free to experiment with other fusion techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CLIP: Zero-Shot Evaluation\n",
    "\n",
    "---\n",
    "The [CLIP](https://arxiv.org/abs/2103.00020) model is pre-trained on a large set of image-caption pairs using a contrastive loss. For this, the images and the text are embedded by two separate encoders and compared with the cosine similarity. Positive image-text pairs should be more similar than negative pairs in the representation space.\n",
    "\n",
    "This allows an evaluation using zero-shot classification, which we will perform in the following using the CIFAR10 dataset. The idea is to classify the images without explicitly training the model on these classes. This is possible due to the capabilities of the text encoder to capture the classes and the coordinated representation space.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/openai/CLIP/main/CLIP.png\" width=1024 height=512/></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "You can see the concept of performing zero-shot classification on the right side of the figure. It is a two-stage process:\n",
    "1. The text encoder is utilized to embed the class names. It is common to insert the class name into a prompt beforehand to provide more context to the model.\n",
    "2. Embed the image with the image encoder and compare the resulting representations with all previously computed text embeddings. Use the cosine similarity to find the best matching class.\n",
    "\n",
    "Let's start by installing the model from the official repository as well as some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "\n",
    "# The CPU should be sufficient for this task\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP was trained with different image encoders, which are listed below. Mainly different sized ResNets and visual transformers are used. Due to limitations in processing power, we'll use the ResNet50 (`RN50`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model, preprocess = clip.load(\"RN50\", device=device, download_root=None)\n",
    "model.eval()\n",
    "\n",
    "# This also provides a useful preprocessing pipeline for the images\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class labels of CIFAR10\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Load the test set of CIFAR10 and add the preprocessing pipeline\n",
    "cifar10 = CIFAR10(root='./data', train=False, download=True, transform=preprocess)\n",
    "\n",
    "# Create a dataloader\n",
    "dl = DataLoader(cifar10, batch_size=64, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "Given two vectors a and b the cosine similarity is defined as:\n",
    "\n",
    "$$\n",
    "    cos(\\mathbf{a}, \\mathbf{b}) = \\dfrac{\\langle \\mathbf{a}, \\mathbf{b} \\rangle}{||\\mathbf{a}||\\cdot ||\\mathbf{b}||} = \\dfrac{ \\mathbf{a}^T\\mathbf{b}}{||\\mathbf{a}||\\cdot ||\\mathbf{b}||}\n",
    "$$\n",
    "\n",
    "This reduces to the standard inner product if your vectors are normalized to unit norm. We can take advantage of this to simplify the calculation of text and image similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Task: Text Embedding (8 Points)\n",
    "We start by embedding all class names using the text encoder. This can be achieved with the function `encode_text` of the CLIP model. The encoder expects text tokens from a [byte-pair encoding](https://huggingface.co/course/chapter6/5?fw=pt), that is implemented in `clip.tokenize`. It maps the raw stings to an int32 tensor. Follow these steps:\n",
    "\n",
    "1. Insert each classname into the template to create a prompt\n",
    "2. Tokenize the prompts with `clip.tokenize`\n",
    "3. Forward the result through the text encoder\n",
    "4. Normalize the embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "##                   START OF YOUR CODE                   ##\n",
    "############################################################\n",
    "# let's start by using no prompt but simply the classname as sanity check\n",
    "template = '{}'\n",
    "\n",
    "# The result is a tensor of shape (1024, 10),\n",
    "# since we have 10 classes and the feature dimension of the text encoder is 1024\n",
    "text_embedding = None\n",
    "\n",
    "\n",
    "# we don't want to calculate gradient during evaluation\n",
    "with torch.no_grad():\n",
    "    pass\n",
    "\n",
    "############################################################\n",
    "##                    END OF YOUR CODE                    ##\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Task: Zero-Shot evaluation (8 Points)\n",
    "Finally we can make a prediction by forwarding the images and compare them to the text embeddings.\n",
    "\n",
    "1. Loop over the dataset\n",
    "2. Create visual embeddings with the image encoder\n",
    "3. Calculate the cosine similarity between the image and text embeddings\n",
    "4. Count the number of correct predictions\n",
    "\n",
    "The accuracy should be around `0.708` if no prompt was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "##                   START OF YOUR CODE                   ##\n",
    "############################################################\n",
    "correct = 0\n",
    "total = len(cifar10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pass\n",
    "\n",
    "print(f\"Accuracy: {correct / total: .3f}\")\n",
    "############################################################\n",
    "##                    END OF YOUR CODE                    ##\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Task: Prompt tuning (4 Points)\n",
    "Create a few other prompts and test the accuracy using the code from the previous tasks. You should increase the accuracy. Report you results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your results:\n",
    "\n",
    "| Prompt | Accuracy | Comment            |\n",
    "|--------| --- |--------------------|\n",
    "| '{}'  | 0.708 | Could be better... |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
