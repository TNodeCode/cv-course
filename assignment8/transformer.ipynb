{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfb5226",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "**Goethe University Frankfurt am Main**\n",
    "\n",
    "Winter Semester 2022/23\n",
    "\n",
    "<br>\n",
    "\n",
    "## *Assignment 8 (Transformer)*\n",
    "\n",
    "---\n",
    "\n",
    "**Points:** 70<br>\n",
    "**Due:** 9.2.2023, 10 am<br>\n",
    "**Contact:** Matthias Fulde ([fulde@cs.uni-frankfurt.de](mailto:fulde@cs.uni-frankfurt.de))<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Your Name:** Tilo-Lars Flasche\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a32fd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "---\n",
    "\n",
    "- [1 Patch Embedding](#1-Patch-Embedding-(10-Points))\n",
    "- [2 Transformer Encoder](#2-Transformer-Encoder-(50-Points))\n",
    "  - [2.1 Fully Connected Network](#2.1-Fully-Connected-Network-(10-Points))\n",
    "  - [2.2 Absolute Position Encoding](#2.2-Absolute-Position-Encoding-(5-Points))\n",
    "  - [2.3 Multihead Attention](#2.3-Multihead-Attention-(25-Points))\n",
    "  - [2.4 Transformer Encoder Layer](#2.4-Transformer-Encoder-Layer-(10-Points))\n",
    "- [3 Vision Transformer](#3-Vision-Transformer-(10-Points))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b45c47",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "We load the PyTorch library that allows us to perform the computations on the graphics card. You can check the GPU support of your environment with the statements below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Check GPU support on your machine.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcbfc2f",
   "metadata": {},
   "source": [
    "Additionally, we load some utility functions to calculate the number of parameters in a network, show samples from the dataset, train a model and check its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006fe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import num_params, show_images, check_accuracy, train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ea58f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Dataset\n",
    "\n",
    "---\n",
    "\n",
    "In this problem set we're working with the MNIST dataset of handwritten digits. The dataset consists of 60000 grayscale images for training and 10000 images for testing. We're loading the dataset from the PyTorch repository and split off a dataset for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of samples per batch.\n",
    "batch_size = 64\n",
    "\n",
    "# Convert input images to tensors.\n",
    "transform = T.ToTensor()\n",
    "\n",
    "# Load and transform training set.\n",
    "data_train = torchvision.datasets.MNIST(\n",
    "    root='./datasets',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Load and transform test set.\n",
    "data_test = torchvision.datasets.MNIST(\n",
    "    root='./datasets',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Split training set into sets for training and validation.\n",
    "data_train, data_val = torch.utils.data.random_split(data_train, [50000, 10000])\n",
    "\n",
    "# Create dataloader for training set.\n",
    "loader_train = torch.utils.data.DataLoader(\n",
    "    data_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Create dataloader for validation set.\n",
    "loader_val = torch.utils.data.DataLoader(\n",
    "    data_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Create dataloader for test set.\n",
    "loader_test = torch.utils.data.DataLoader(\n",
    "    data_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4890a353",
   "metadata": {},
   "source": [
    "Let's have a look at how the images in the dataset look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(loader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71eba12",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Exercises\n",
    "\n",
    "---\n",
    "\n",
    "The goal in this assignment is to create a vision transformer model for classification.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1 Patch Embedding (10 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Transformers work on sequential data. In particular, the model expects its input to be a sequence of vectors. Since images don't naturally fit this specification, we first have to write some code for converting an image to a sequence of feature vectors.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1.1 Task\n",
    "\n",
    "Complete the definition of the `PatchEmbed` class below.\n",
    "\n",
    "The module should take a batch of images, represented with channels before spatial dimensions, partition each image into non-overlapping patches of size `patch_size`, and linearly project each patch into an embedding vector with the dimension `embed_dim`. Optionally, the resulting vectors should be normalized.\n",
    "\n",
    "We assume that if a normalization module is provided, it's going to be a layer norm. Since we only want to compute the statistics across the last dimension, pass `embed_dim` to the constructor when conditionally creating an instance.\n",
    "\n",
    "In case of grayscale images such as those from the MNIST dataset, there might not be a separate channel dimension. You should check in the `forward` method the number of dimensions of the input and if the channel dimension is missing, add a dimension of size $1$ in the second position after the batch dimension.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1.2 Solution\n",
    "\n",
    "Write your solution in the delimited areas below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size, channels=1, patch_size=4, embed_dim=96, norm_layer=None):\n",
    "        \"\"\"\n",
    "        Create image to patch embedding module.\n",
    "\n",
    "        Parameters:\n",
    "            - img_size (int): Image size.\n",
    "            - channels (int) [1]: Number of input channels.\n",
    "            - patch_size (int) [4]: Patch token size.\n",
    "            - embed_dim (int) [96]: Size of embedding vectors.\n",
    "            - norm_layer (nn.Module|None) [None]: Normalization layer.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ############################################################\n",
    "        ###                  START OF YOUR CODE                  ###\n",
    "        ############################################################\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.img_size = img_size\n",
    "        self.channels = channels\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.norm_layer = norm_layer\n",
    "        \n",
    "        # Layers\n",
    "        self.embedding = nn.Linear(patch_size*patch_size, embed_dim)\n",
    "\n",
    "        ############################################################\n",
    "        ###                   END OF YOUR CODE                   ###\n",
    "        ############################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Create patch embeddings from input images.\n",
    "\n",
    "        Parameters:\n",
    "            - x (torch.Tensor): (batch_size, [channels,] height, width).\n",
    "\n",
    "        Returns:\n",
    "            - x (torch.Tensor): (batch_size, num_patches, embed_dim).\n",
    "\n",
    "        \"\"\"\n",
    "        ############################################################\n",
    "        ###                  START OF YOUR CODE                  ###\n",
    "        ############################################################\n",
    "\n",
    "        # If there is no explicit dimension for the channel then add one\n",
    "        if (len(x.shape) == 3):\n",
    "            x = x.unsqueeze(dim=1)\n",
    "            \n",
    "        # Extrakt shape informtion\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        \n",
    "        # Split images into patches (patches will be of shape (batch_size,n_patches,channels,patch_size^2)\n",
    "        # Each list items contains the same region of each image in the batch\n",
    "        size = self.patch_size # patch size\n",
    "        n_patches = (self.img_size // self.patch_size)**2 * self.channels\n",
    "        patches = x.unfold(2, size, size).unfold(3, size, size).reshape(batch_size, n_patches, size*size)\n",
    "        \n",
    "        # Embed each patch (result will be of shape (batch_size,n_patches,channels,embed_dim))\n",
    "        embeddings = self.embedding(patches)\n",
    "        \n",
    "        if self.norm_layer:\n",
    "            embeddings = self.norm_layer(embeddings)\n",
    "\n",
    "        ############################################################\n",
    "        ###                   END OF YOUR CODE                   ###\n",
    "        ############################################################\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22d8c1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2 Transformer Encoder (50 Points)\n",
    "\n",
    "---\n",
    "\n",
    "For the vision transformer we're about to create, we get rid of the decoder part from the original transformer description. Our transformer network will just be a stack of encoder blocks. Each transformer encoder block is composed of two sublayers, namely a multihead self-attention sublayer and a small fully connected network with one hidden layer.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.1 Fully Connected Network (10 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin with the fully connected network for the encoder blocks.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.1.1 Task\n",
    "\n",
    "Complete the definition of the `FullyConnectedNetwork` class below.\n",
    "\n",
    "As mentioned above, this network consists of two layers, a hidden layer and an output layer. The dimension of the output vectors must match the dimension of the input. In order to determine the size of the intermediate vectors that are produced by the hidden layer, we use a `scale` parameter, which is multiplied with `in_features`. The result must be cast to integer before passing to the appropriate module for creating the linear layer.\n",
    "\n",
    "The output of the hidden layer is passed through a non-linearity that is given as a module with the `activation` parameter of the constructor. The output layer is just linear, without activation. Optionally, dropout should be applied to the activations of the hidden layer and to the final output.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.1.2 Solution\n",
    "\n",
    "Write your solution in the delimited areas below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf5d12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, scale=2.0, activation=nn.LeakyReLU, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Create fully connected network for transformer block.\n",
    "\n",
    "        Parameters:\n",
    "            - in_features (int): Size of input vectors.\n",
    "            - scale (float) [2.0]: Scaler for size of hidden vectors.\n",
    "            - activation (nn.Module) [nn.LeakyReLU]: Activation function for hidden layer.\n",
    "            - dropout (float) [0.0]: Probability for dropout.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ############################################################\n",
    "        ###                  START OF YOUR CODE                  ###\n",
    "        ############################################################\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.in_features = in_features\n",
    "        self.scale = scale\n",
    "        self.hidden_size = int(in_features*scale)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Layers\n",
    "        self.fc1 = nn.Linear(self.in_features, self.hidden_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.in_features)\n",
    "        self.activation = activation()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        ############################################################\n",
    "        ###                   END OF YOUR CODE                   ###\n",
    "        ############################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute forward pass through network.\n",
    "\n",
    "        Parameters:\n",
    "            - x (torch.Tensor): (batch_size, num_patches, in_features).\n",
    "\n",
    "        Returns:\n",
    "            - x (torch.Tensor): (batch_size, num_patches, in_features).\n",
    "\n",
    "        \"\"\"\n",
    "        ############################################################\n",
    "        ###                  START OF YOUR CODE                  ###\n",
    "        ############################################################\n",
    "\n",
    "        # Run inputs through the first layer\n",
    "        x = self.dropout(self.activation(self.fc1(x)))\n",
    "        # Run the hidden layer outputs through the second layer\n",
    "        x = self.dropout(self.fc2(x))\n",
    "\n",
    "        ############################################################\n",
    "        ###                   END OF YOUR CODE                   ###\n",
    "        ############################################################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded3c9c7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.2 Absolute Position Encoding (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Since transformers have no built-in mechanism to make sense of the ordering of the sequence elements, we need some kind of positional encoding for our model. In the lecture we only discussed the fixed function position encoding that was used in the original description of the transformer architecture, however, for our model, we want to use a learnable absolute position bias instead.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.2.1 Task\n",
    "\n",
    "Complete the definition of the `AbsolutePositionEncoding` class below.\n",
    "\n",
    "In the constructor, create a randomly initialized parameter matrix with shape $(\\text{head_dim, num_tokens})$ and multiply the parameter values with the provided `scale` factor. The same parameters are shared across all attention heads.\n",
    "\n",
    "In the `forward` method, we compute the position bias as the product\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    \\mathbf{B} = \\mathbf{Q}\\mathbf{W}^B\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where $\\mathbf{Q}$ is a query matrix and $\\mathbf{W}^B$ is the parameter matrix created in the constructor. Hence, the position bias has the same shape as the attention map, the product between the queries and the keys, that is $(\\text{num_tokens, num_tokens})$. So they can be added together.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.2.2 Solution\n",
    "\n",
    "Write your solution in the delimited areas below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c072317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsolutePositionEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, num_tokens, head_dim, scale):\n",
    "        \"\"\"\n",
    "        Create learnable absolute position bias. \n",
    "\n",
    "        Parameters:\n",
    "            num_tokens (int): Number of elements in the sequence.\n",
    "            head_dim (int): Size of the query vectors.\n",
    "            scale (float): Scaler for query vectors.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ############################################################\n",
    "        ###                  START OF YOUR CODE                  ###\n",
    "        ############################################################\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.num_tokens = num_tokens\n",
    "        self.head_dim = head_dim\n",
    "        self.scale = scale\n",
    "        \n",
    "        # Layers\n",
    "        self.weights = nn.Parameter(torch.randn(head_dim, num_tokens))\n",
    "\n",
    "        ############################################################\n",
    "        ###                   END OF YOUR CODE                   ###\n",
    "        ############################################################\n",
    "\n",
    "    def forward(self, q):\n",
    "        \"\"\"\n",
    "        Compute absolute position bias.\n",
    "\n",
    "        Parameters:\n",
    "            - q (torch.Tensor): Query vectors ([batch_size, num_heads,] num_tokens, head_dim).\n",
    "\n",
    "        Returns:\n",
    "            - bias (torch.Tensor): ([batch_size, num_heads,] num_tokens, num_tokens).\n",
    "\n",
    "        \"\"\"\n",
    "        ############################################################\n",
    "        ###                  START OF YOUR CODE                  ###\n",
    "        ############################################################\n",
    "\n",
    "        bias = q @ (self.weights * self.scale)\n",
    "\n",
    "        ############################################################\n",
    "        ###                   END OF YOUR CODE                   ###\n",
    "        ############################################################\n",
    "        return bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2879cf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.3 Multihead Attention (25 Points)\n",
    "\n",
    "---\n",
    "\n",
    "The core component of the transformer is the multihead attention sublayer, where each feature vector is provided with the contextual information from all other elements in the sequence. For a single attention head, we compute\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    \\text{Attention}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})\n",
    "    =\n",
    "    \\text{Softmax}\\left(\n",
    "        \\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d}} + \\mathbf{B}\n",
    "    \\right)\n",
    "    \\mathbf{V}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where $\\mathbf{Q}, \\mathbf{K}$, and $\\mathbf{V}$ are the query, key, and value matrices, $\\mathbf{B}$ is the position bias, and $d$ is the dimension of the queries, keys, and values. For our particular implementation, we will assume that the feature vectors all have the same dimension and that this dimension is determined by dividing the dimension of the embedding vectors by the number of attention heads.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.3.1 Task\n",
    "\n",
    "Complete the definition of the `MultiheadAttention` class below.\n",
    "\n",
    "The `embed_dim` parameter of the constructor determines the size of the input vectors. The `num_tokens` parameter specifies the number of elements in the sequence. This information is needed for constructing the parameters of the position bias.\n",
    "\n",
    "The number of attention heads to use is given by the `num_heads` parameter. As mentioned above, we compute the dimensions of the queries, keys, and values as\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    \\text{head_dim} = \\frac{\\text{embed_dim}}{\\text{num_heads}},\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "assuming that the embedding dimension is divisible by the number of attention heads. This way we can facilitate the implementation and make it more efficient, saving the step of concatenating the output matrices from the different attention heads before the final projection.\n",
    "\n",
    "Furthermore, we'll make the scaling factor a hyperparameter of our model, using the `scale` parameter of the constructor. In case no value is provided, we compute the factor as\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    \\text{scale} = \\frac{1}{\\sqrt{\\text{head_dim}}},\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "consitent with the description above. Finally, for regularization of our model, we optionally want to apply dropout both to the attention map after the application of softmax, and to the result of the final projection.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.3.2 Solution\n",
    "\n",
    "Write your solution in the delimited areas below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0644d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_tokens, num_heads, scale=None, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Create multihead attention layer.\n",
    "\n",
    "        Parameters:\n",
    "            - embed_dim (int): Dimension of input vectors.\n",
    "            - num_tokens (int): Number of tokens in the sequence.\n",
    "            - num_heads (int): Number of attention heads.\n",
    "            - scale (float|None) [None]: Scaler for query vectors.\n",
    "            - dropout (float) [0.0]: Probability for dropout.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ############################################################\n",
    "        ###                  START OF YOUR CODE                  ###\n",
    "        ############################################################\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_tokens = num_tokens\n",
    "        self.scale = scale if scale is not None else (1 / head_dim**0.5)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Layers\n",
    "        self.pos_enc = AbsolutePositionEncoding(self.num_tokens, self.head_dim, self.scale)\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "\n",
    "        ############################################################\n",
    "        ###                   END OF YOUR CODE                   ###\n",
    "        ############################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute forward pass through attention layer.\n",
    "\n",
    "        Parameters:\n",
    "            - x (torch.Tensor): (batch_size, num_patches, embed_dim)\n",
    "\n",
    "        Returns:\n",
    "            - x (torch.Tensor): (batch_size, num_patches, embed_dim)\n",
    "\n",
    "        \"\"\"\n",
    "        ############################################################\n",
    "        ###                  START OF YOUR CODE                  ###\n",
    "        ############################################################\n",
    "\n",
    "        # Input dimensions\n",
    "        batch_size, num_patches, embed_dim = x.shape\n",
    "        \n",
    "        # Epand the input dimensions so that embed_dim = num_heads x head_dim\n",
    "        q = x.reshape(batch_size, num_patches, self.num_heads, self.head_dim)\n",
    "        k = x.reshape(batch_size, num_patches, self.num_heads, self.head_dim)\n",
    "        v = x.reshape(batch_size, num_patches, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Calculate the weights by matrix multiplication\n",
    "        query = self.queries(q)\n",
    "        key = self.keys(k)\n",
    "        value = self.values(v)\n",
    "        \n",
    "        # ompute energy\n",
    "        energy = torch.einsum(\"bnhd,bxhd->bhnx\", [query, key]) \n",
    "        \n",
    "        # Calculate AbsolutePositionEncoding\n",
    "        bias = self.pos_enc(query)\n",
    "        \n",
    "        # Calculate Attention\n",
    "        energy = energy / (self.scale) + bias.permute(0, 2, 1, 3)\n",
    "        attention = torch.softmax(energy, 3)\n",
    "        attention @= v.permute(0, 2, 1, 3)\n",
    "\n",
    "        # Transform the output back in input shape\n",
    "        x = attention.permute(0, 2, 1, 3).reshape(batch_size, num_patches, embed_dim)\n",
    "        \n",
    "        ############################################################\n",
    "        ###                   END OF YOUR CODE                   ###\n",
    "        ############################################################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e733c2b4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.4 Transformer Encoder Layer (10 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Now we have everything together to implement a transformer encoder block.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.4.1 Task\n",
    "\n",
    "Complete the definition of the `TransformerEncoderLayer` class below.\n",
    "\n",
    "For the attention and fully connected sublayers, create instances of the `MultiheadAttention` and `FullyConnectedNetwork` classes, passing the necessary parameters to the constructors. We use layer norm for normalization. When creating the normalization layers, pass in the value provided for the `embed_dim` parameter.\n",
    "\n",
    "The transformer encoder layer should work both with the pre normalization and post normalization layouts. Which layout should be used is determined by the `norm_first` parameter of the constructor. If you don't remember the difference, review the slides of the transformer lecture.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.4.2 Solution\n",
    "\n",
    "Write your solution in the delimited areas below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d09f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_tokens, num_heads, scale=2.0, activation=nn.LeakyReLU, norm_first=True):\n",
    "        \"\"\"\n",
    "        Create transformer encoder layer.\n",
    "\n",
    "        Parameters:\n",
    "            - embed_dim (int): Dimension of input vectors.\n",
    "            - num_tokens (int): Number of elements in the sequence.\n",
    "            - num_heads (int): Number of attention heads.\n",
    "            - scale (float) [2.0]: Scaler for hidden dimension of fully connected network.\n",
    "            - activation (nn.Module) [nn.LeakyReLU]: Activation function of intermediate layer.\n",
    "            - norm_first (bool) [True]: Pre or post norm layout.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ############################################################\n",
    "        ###                  START OF YOUR CODE                  ###\n",
    "        ############################################################\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_tokens = num_tokens\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = scale\n",
    "        self.activation = activation\n",
    "        self.norm_first = norm_first\n",
    "        \n",
    "        # Layers\n",
    "        self.attention = MultiheadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_tokens=num_tokens,\n",
    "            num_heads=num_heads,\n",
    "            scale=scale\n",
    "        )\n",
    "        self.fc = FullyConnectedNetwork(\n",
    "            in_features=embed_dim,\n",
    "            scale=2.0,\n",
    "            activation=activation,\n",
    "            dropout=0.0\n",
    "        )        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        ############################################################\n",
    "        ###                   END OF YOUR CODE                   ###\n",
    "        ############################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute forward pass through transformer encoder layer.\n",
    "\n",
    "        Parameters:\n",
    "            - x (torch.Tensor): (batch_size, num_patches, embed_dim)\n",
    "\n",
    "        Returns:\n",
    "            - x (torch.Tensor): (batch_size, num_patches, embed_dim)\n",
    "\n",
    "        \"\"\"\n",
    "        ############################################################\n",
    "        ###                  START OF YOUR CODE                  ###\n",
    "        ############################################################\n",
    "\n",
    "        if self.norm_first:\n",
    "            x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        ########## ##################################################\n",
    "        ###                   END OF YOUR CODE                   ###\n",
    "        ############################################################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf91d4ae",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3 Vision Transformer (10 Points)\n",
    "\n",
    "---\n",
    "\n",
    "The last step in our implementation is to create the transformer encoder network for classification.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3.1 Task\n",
    "\n",
    "Complete the definition of the `VisionTransformer` class below.\n",
    "\n",
    "The `num_layers` parameter of the constructor determines the number of transformer encoder layers to use in the network. All of these layers are built the same, using the values provided for `embed_dim` and `num_heads`. Given the size of the input images and the size of the image patches created by the patch embedding layer, compute the number of elements in the sequence.\n",
    "\n",
    "Besides the patch embedding layer and the transformer encoder layers, we need another component to actually perform the classification task. As in the original vision transformer implementation, we'll use a special class token that is added to the beginning of the sequence for each input image. After the final transformer encoder layer, this token is extracted and passed to a linear layer that maps the token of size `embed_dim` to a vector of size `num_classes`, which is the final output of the model.\n",
    "\n",
    "So, assuming we use a patch size of $4$, as is the default value in the implementation below, then for MNIST we would get a sequence length of\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    \\text{num_patches} = \\left(\\frac{28}{4}\\right)^2 = 49.\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Accordingly, adding the class token for each image would increase the sequence length to $50$. The class token that is added to the sequence is itself a learnable parameter, hence you should create a parameter of the appropriate size and initialize it randomly in the constructor.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3.2 Solution\n",
    "\n",
    "Write your solution in the delimited areas below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, img_size, channels, patch_size=4, num_layers=4, embed_dim=96, num_heads=8):\n",
    "        \"\"\"\n",
    "        Create vision transformer for classification.\n",
    "\n",
    "        Parameters:\n",
    "            - num_classes (int): Number of classes.\n",
    "            - img_size (int): Image height and width.\n",
    "            - channels (int): Number of input channels.\n",
    "            - patch_size (int) [4]: Patch token size.\n",
    "            - num_layers (int) [4]: Number of transformer encoder layers.\n",
    "            - embed_dim (int) [96]: Size of embedding vectors.\n",
    "            - num_heads (int) [8]: Number of attention heads.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ############################################################\n",
    "        ###                  START OF YOUR CODE                  ###\n",
    "        ############################################################\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.channels = channels\n",
    "        self.patch_size = patch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_tokens = (img_size // patch_size)**2\n",
    "        \n",
    "        # Layers\n",
    "        self.embedding = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            channels=channels,\n",
    "            patch_size=patch_size,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "        self.encoders = nn.ModuleList([TransformerEncoderLayer(\n",
    "            embed_dim=embed_dim,\n",
    "            num_tokens=self.num_tokens+1,\n",
    "            num_heads=num_heads,\n",
    "        ) for _ in range(num_layers)])\n",
    "        self.class_token = nn.Parameter(torch.FloatTensor(embed_dim))\n",
    "        self.cls = nn.Linear(embed_dim, num_classes, bias=False)\n",
    "\n",
    "        ############################################################\n",
    "        ###                   END OF YOUR CODE                   ###\n",
    "        ############################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute forward pass through network.\n",
    "\n",
    "        Parameters:\n",
    "            - x (torch.Tensor): (batch_size, channels, height, width)\n",
    "\n",
    "        Returns:\n",
    "            - x (torch.Tensor): (batch_size, num_classes)\n",
    "\n",
    "        \"\"\"\n",
    "        ############################################################\n",
    "        ###                  START OF YOUR CODE                  ###\n",
    "        ############################################################\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        class_tokens = torch.ones(batch_size, 1, self.embed_dim) * self.class_token\n",
    "        x = self.embedding(x)\n",
    "        x = torch.cat([class_tokens, x], dim=1)\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        x = x[:, 0, :].squeeze(1)\n",
    "        x = self.cls(x)\n",
    "\n",
    "        ############################################################\n",
    "        ###                   END OF YOUR CODE                   ###\n",
    "        ############################################################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f60859",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.3 Results\n",
    "\n",
    "For training and evaluating our model we can use the utility functions imported on top of the notebook. We create a vision transformer with two layers, otherwise using the default parameter values of the modules defined above. You can check the number of parameters of the model with the statements below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49bf318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model.\n",
    "model = VisionTransformer(num_classes=10, img_size=28, channels=1, num_layers=2)\n",
    "\n",
    "# Show number of parameters.\n",
    "print(num_params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2890a4",
   "metadata": {},
   "source": [
    "Now let's train the model for some epochs using cross entropy loss and Adam optimization. If you implemented everything correctly, you should expect to see a very high accuracy, well above $90 \\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3676fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function and optimizer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model for some epochs.\n",
    "history = train(model, criterion, optimizer, loader_train, loader_val, num_epochs=5)\n",
    "\n",
    "# Check test accuracy.\n",
    "print(f'\\nTest accuracy: {check_accuracy(model, loader_test):.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa64ed-d64f-4a81-966b-bc233c63aa40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
