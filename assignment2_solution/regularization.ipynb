{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfb5226",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "**Goethe University Frankfurt am Main**\n",
    "\n",
    "Winter Semester 2022/23\n",
    "\n",
    "<br>\n",
    "\n",
    "## *Assignment 2 (Regularization)*\n",
    "\n",
    "---\n",
    "\n",
    "**Points:** 10<br>\n",
    "**Due:** 10.11.2022, 10 am<br>\n",
    "**Contact:** Matthias Fulde ([fulde@cs.uni-frankfurt.de](mailto:fulde@cs.uni-frankfurt.de))<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Your Name:** Tilo-Lars Flasche\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64171c5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "---\n",
    "\n",
    "- [1 L1 Regularization](#1-L1-Regularization-(5-Points))\n",
    "  - [1.1 Implementation](#1.1-Implementation-(3-Points))\n",
    "  - [1.2 Explanation](1.2-Explanation-(2-Points))\n",
    "- [2 L2 Regularization](#2-L2-Regularization-(5-Points))\n",
    "  - [2.1 Implementation](#2.1-Implementation-(3-Points))\n",
    "  - [2.2 Explanation](#2.2-Explanation-(2-Points))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31afd2c5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we use the only the **NumPy** library.\n",
    "\n",
    "We import definitions of regularizers from the `regularization.py` module and enable autoreload, so that the imported functions are automatically updated whenever the code is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce3f4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from regularization import L1_reg, L2_reg\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b1f2db",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Exercises\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dbf55f",
   "metadata": {},
   "source": [
    "### 2 L1 Regularization (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "In this exercise we want to implement **L1 regularization**. Here, the regularizer is the absolute value of the model's weights, defined as\n",
    "\n",
    "$$\n",
    "    R(W) = \\sum_{i=1}^D \\sum_{j=1}^K \\vert W_{i,j} \\vert.\n",
    "$$\n",
    "\n",
    "In order to control the effects of the regularization term, we introduce the regularization strength $\\lambda$ as a hyperparameter. The complete loss for our model is then the sum of the data loss $\\mathcal{L}$ and the regularization loss $R$, that is\n",
    "\n",
    "$$\n",
    "    J(W) = \\mathcal{L}(W) + \\lambda R(W).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf6d12b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.1 Implementation (3 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Complete the definition of the `L1_reg` function in the `regularization.py` file.\n",
    "\n",
    "The function takes a parameter matrix $W$ of shape $(D+1, K)$, where $K$ is the number of categories and $D$ is the dimension of the inputs. The last row is assumed to be the bias. The second parameter is the regularization strength.\n",
    "\n",
    "The function should return a tuple $(R, dW)$ with the regularization loss $R$, computed only for the weights and not the bias, and the gradient of the loss $dW$ with respect to the parameters. So the loss $R$ is a scalar and $dW$ has the same shape as $W$.\n",
    "\n",
    "Use only vectorized NumPy operations for the implementation. No loops are allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74b35d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Test 1.1.1\n",
    "\n",
    "To test your implementation, you can run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a5a86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dummy parameters.\n",
    "W = np.array([\n",
    "    [ 1.2,  3.6,  8.1],\n",
    "    [ 4.0, -1.0,  3.6],\n",
    "    [-9.6,  2.5, -6.3],\n",
    "    [ 3.5, -7.2, -2.0]\n",
    "])\n",
    "\n",
    "# Compute regularization loss.\n",
    "R, dW = L1_reg(W, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8254948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Compare loss.\n",
    "loss_equal = abs(R - 19.95) < 1e-5\n",
    "\n",
    "# Compare derivatives.\n",
    "grad_equal = np.array_equal(dW, np.array([\n",
    "    [ 0.5,  0.5,  0.5],\n",
    "    [ 0.5, -0.5,  0.5],\n",
    "    [-0.5,  0.5, -0.5],\n",
    "    [ 0.0,  0.0,  0.0]\n",
    "]))\n",
    "\n",
    "# Show results.\n",
    "print(loss_equal and grad_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af3e9c",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "*Write your answer here.*\n",
    "\n",
    "First let's look at how R is calculated. It is the sum of all components of the weight matrix except the last row.\n",
    "$$ R = w_{11} + ... + w_{1k} + w_{21} + ... w_{2k} + ... + w_{d1} + ... + w_{dk} $$\n",
    "\n",
    "If we derive this sum with respect to $w_{ij}$ the derivative is just one.\n",
    "\n",
    "$$ \\frac{\\partial R}{\\partial w_{ij}} = \\begin{cases} 1 & w_{ij} \\ge 0 \\\\ -1 & w_{ij} < 0 \\end{cases} $$\n",
    "\n",
    "The derivative of $R$ with respect to $W$ is a matrix where each components is one (if $w_{ij} \\ge 0$) or minus one (if $w_{ij} < 0$) \n",
    "\n",
    "$$ \\frac{\\partial R}{\\partial W} = \\begin{pmatrix}\n",
    "\\frac{\\partial R}{\\partial w_{11}} & \\dots & \\frac{\\partial R}{\\partial w_{1k}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial R}{\\partial w_{d1}} & \\dots & \\frac{\\partial R}{\\partial w_{dk}}\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "If we now multiply $\\frac{\\partial R}{\\partial w_{ij}}$ with $r$ the result is just $r$\n",
    "\n",
    "$$ r \\cdot \\frac{\\partial R}{\\partial w_{ij}} = \\begin{cases} r & w_{ij} \\ge 0 \\\\ -r & w_{ij} < 0 \\end{cases} $$\n",
    "\n",
    "If we multiply $\\frac{\\partial R}{\\partial W}$ with $r$ we get a matrix where each components is $r$ or $-r$\n",
    "\n",
    "$$ r \\cdot \\frac{\\partial R}{\\partial W} = r \\cdot \\begin{pmatrix}\n",
    "\\frac{\\partial R}{\\partial w_{11}} & \\dots & \\frac{\\partial R}{\\partial w_{1k}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial R}{\\partial w_{d1}} & \\dots & \\frac{\\partial R}{\\partial w_{dk}}\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "r \\cdot \\frac{\\partial R}{\\partial w_{11}} & \\dots & r \\cdot \\frac{\\partial R}{\\partial w_{1k}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "r \\cdot \\frac{\\partial R}{\\partial w_{d1}} & \\dots & r \\cdot \\frac{\\partial R}{\\partial w_{dk}}\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5845b93",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2 L2 Regularization (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "In this exercise we want to implement **L2 regularization**. Here, the regularizer is the squared euclidean distance of the model's weights, defined as\n",
    "\n",
    "$$\n",
    "    R(W) = \\sum_{i=1}^D \\sum_{j=1}^K W_{i,j}^2.\n",
    "$$\n",
    "\n",
    "Again, we have the regularization strength $\\lambda$ as an additional hyperparameter, controlling by how much we restrict the model's parameters. The complete loss for our model is the sum of the data loss $\\mathcal{L}$ and the regularization loss $R$, that is\n",
    "\n",
    "$$\n",
    "    J(W) = \\mathcal{L}(W) + \\lambda R(W).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2e154",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.1 Implementation (3 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Complete the definition of the `L2_reg` function in the `regularization.py` file.\n",
    "\n",
    "The function takes a parameter matrix $W$ of shape $(D+1, K)$, where $K$ is the number of categories and $D$ is the dimension of the inputs. The last row is assumed to be the bias. The second parameter is the regularization strength.\n",
    "\n",
    "The function should return a tuple $(R, dW)$ with the regularization loss $R$, computed only for the weights and not the bias, and the gradient of the loss $dW$ with respect to the parameters. So the loss $R$ is a scalar and $dW$ has the same shape as $W$.\n",
    "\n",
    "Use only vectorized NumPy operations for the implementation. No loops are allowed.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "First let's look at how R is calculated. It is the sum of all squared components of the weight matrix except the last row.\n",
    "\n",
    "$$ R = w_{11}^2 + ... + w_{1k}^2 + w_{21}^2 + ... w_{2k}^2 + ... + w_{d1}^2 + ... + w_{dk}^2 $$\n",
    "\n",
    "If we derive this sum with respect to $w_{ij}$ the derivative is two times $w_{ij}$.\n",
    "\n",
    "$$ \\frac{\\partial R}{\\partial w_{ij}} = 2 w_{ij} $$\n",
    "\n",
    "The regularization term does not depend on the bias values, so the derivative of $R$ with respect to the $bias$ is zero\n",
    "\n",
    "$$ \\frac{\\partial R}{\\partial w_{D+1,j}} = 0 $$\n",
    "\n",
    "The derivative of $R$ with respect to $W$ is a matrix where each components is two times its original value\n",
    "\n",
    "$$ \\frac{\\partial R}{\\partial W} = \\begin{pmatrix}\n",
    "\\frac{\\partial R}{\\partial w_{11}} & \\dots & \\frac{\\partial R}{\\partial w_{1k}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial R}{\\partial w_{d1}} & \\dots & \\frac{\\partial R}{\\partial w_{dk}}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "2 w_{11} & \\dots & 2 w_{1k} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "2 w_{d1} & \\dots & 2 w_{dk}\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "If we now multiply $\\frac{\\partial R}{\\partial w_{ij}}$ with $r$ the result is $2rw_{ij}$\n",
    "\n",
    "$$ r \\cdot \\frac{\\partial R}{\\partial w_{ij}} = 2rw_{ij} $$\n",
    "\n",
    "If we multiply $\\frac{\\partial R}{\\partial W}$ with $r$ we get a matrix where each components is $r$ or $-r$\n",
    "\n",
    "$$ r \\cdot \\frac{\\partial R}{\\partial W} = r \\cdot \\begin{pmatrix}\n",
    "\\frac{\\partial R}{\\partial w_{11}} & \\dots & \\frac{\\partial R}{\\partial w_{1k}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial R}{\\partial w_{d1}} & \\dots & \\frac{\\partial R}{\\partial w_{dk}}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "r \\cdot \\begin{pmatrix}\n",
    "2 w_{11} & \\dots & 2 w_{1k} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "2 w_{d1} & \\dots & 2 w_{dk}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "2r w_{11} & \\dots & 2r w_{1k} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "2r w_{d1} & \\dots & 2r w_{dk}\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec81b98",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2.1.1 Test\n",
    "\n",
    "To test your implementation, you can run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45271089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dummy parameters.\n",
    "W = np.array([\n",
    "    [ 1.2,  3.6,  8.1],\n",
    "    [ 4.0, -1.0,  3.6],\n",
    "    [-9.6,  2.5, -6.3],\n",
    "    [ 3.5, -7.2, -2.0]\n",
    "])\n",
    "\n",
    "# Compute regularization loss.\n",
    "R, dW = L2_reg(W, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8572fbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Compare loss.\n",
    "loss_equal = abs(R - 124.035) < 1e-5\n",
    "\n",
    "# Compare gradient.\n",
    "grad_equal = np.array_equal(dW, [\n",
    "    [ 1.2,  3.6,  8.1],\n",
    "    [ 4.0, -1.0,  3.6],\n",
    "    [-9.6,  2.5, -6.3],\n",
    "    [ 0.0,  0.0,  0.0]\n",
    "])\n",
    "\n",
    "# Show results.\n",
    "print(loss_equal and grad_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326f51de",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.2 Explanation (2 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Briefly describe in your own words how the L2 regularization affects the parameters of the model.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be37d81",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "*Write your answer here.*\n",
    "\n",
    "L2 regularization punishes large weights in a non-linear way. If the weight doubles in contributes four time more than before to the regularization error. This leads to small weights when we train the model and backpropagate the error. Weights can not become zero when using L2 regularization.\n",
    "\n",
    "As a numerical example, defining an input vector $x=(1,1,1,1)$ and two weight vectors $w_1 = (1,0,0,0)$ and $w_2 = (0.25, 0.25, 0.25, 0.25)$ then\n",
    "$$ w_1^Tx = w_2^T x = 1 $$\n",
    "but $w_2$ would be the preferred solution when L2 regularization is applied, since\n",
    "$$ 1 = \\sum\\limits_{i=1}^4 w_{1,i}^2 > \\sum\\limits_{i=1}^4 w_{2,i}^2 = 0.25 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5d7ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
