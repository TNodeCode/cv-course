{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "**Goethe University Frankfurt am Main**\n",
    "\n",
    "Winter Semester 2022/23\n",
    "\n",
    "<br>\n",
    "\n",
    "## *Assignment 6 (U-Net)*\n",
    "\n",
    "---\n",
    "\n",
    "**Points:** 110<br>\n",
    "**Due:** 14.12.2022, 10 am<br>\n",
    "**Contact:** Timothy Schauml√∂ffel (Discord)<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Your Name:** Tilo-Lars Flasche\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "---\n",
    "\n",
    "- [1 Dataset](#1-Dataset)\n",
    "    - [1.1 Augmentations](#1.1-Augmentations-(15-Points))\n",
    "- [2 Model](#2-Model-(20-Points))\n",
    "- [3 Loss](#3-Loss-(20-Points))\n",
    "- [4 Metric](#4-Metric-(20-Points))\n",
    " - [4.1 Accuracy](#4.1-Accuracy-(5-Points))\n",
    "- [5 Training](#5-Training-(20-Points))\n",
    "- [6 Mask RCNN](#6-Mask-RCNN-(10-Points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "## Setup Code\n",
    "Before getting started, we need to run some boilerplate code to set up our environment, same as previous assignments. You\"ll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (0.19.3)\n",
      "Requirement already satisfied: numpy>=1.11.1 in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (1.23.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (5.4.1)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (4.6.0.66)\n",
      "Requirement already satisfied: scipy in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (1.9.0)\n",
      "Requirement already satisfied: qudida>=0.0.4 in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qudida>=0.0.4->albumentations) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qudida>=0.0.4->albumentations) (4.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2.8.7)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (9.2.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2022.10.10)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2.21.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations) (3.0.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tilof\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# should be installed in Colab\n",
    "!pip install -U albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab Setup\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a 2022WI folder and put all the files under A5 folder, then \"2022WI/A4\"\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = ''\n",
    "ROOT_PATH = Path(\"drive\", \"My Drive\") / GOOGLE_DRIVE_PATH_AFTER_MYDRIVE\n",
    "print(list(ROOT_PATH.iterdir()))\n",
    "\n",
    "# Add to sys so we can import .py files.\n",
    "sys.path.append(str(ROOT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from helper import plot_segmentation, plot_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a few constants related to data loading.\n",
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SHAPE = (224, 224)\n",
    "MEAN = (0.485, 0.456, 0.406)\n",
    "STD = (0.229, 0.224, 0.225)\n",
    "NUM_WORKERS = multiprocessing.cpu_count()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Good to go!\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 1 Dataset\n",
    "\n",
    "---\n",
    "\n",
    "## Oxford-IIIT Pet\n",
    "---\n",
    "In the last exercise we used bounding boxes to predict the location and class of objects in images. In this exercise\n",
    "we want to train a model on the task of semantic segmentation.\n",
    "\n",
    "In semantic segmentation we want to predict a mask for each image. Each pixel in the mask should be labeled with the\n",
    "class of the object it belongs to. Consequently the mask has the same size as the image and each pixel is labeled with\n",
    "an integer between 0 and the number of classes - 1.\n",
    "\n",
    "We will use [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/), which contains images of cats and dogs.\n",
    "The dataset is available in torchvision and we can load it with torchvision.datasets.OxfordPets. The dataset contains\n",
    "annotations for classification and segmentation masks. We will use the segmentation masks for this exercise.\n",
    "\n",
    "Run the following cell to download the dataset automatically and show one example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = '/media/ts/SSD_ubuntu/datasets/'\n",
    "dataset = datasets.OxfordIIITPet(root=ROOT_PATH, target_types='segmentation', download=True)\n",
    "print(dataset)\n",
    "\n",
    "image, mask = dataset[0]\n",
    "plot_segmentation(image, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains three classes (we will scale the index to start at 0 in the dataloader for you):\n",
    "   * index 1 = foreground\n",
    "   * index 2 = background\n",
    "   * index 3 = border\n",
    "\n",
    "Usually we want to ignore the border class and only predict foreground and background. To make this task a bit more interesting\n",
    "we will also predict the border class. This means we will have **3 classes in total**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Augmentations (15 Points)\n",
    "Since the dataset is very small we will add some data augmentation to the training set. For this we introduce the library\n",
    "**albumentations**, which contains a large collection of augmentation and is very popular in the computer vision community.\n",
    "One of the advantages of albumentations is that it applies the same augmentation to the image and the mask if necessary.\n",
    "\n",
    "**TODO**: Make yourself familiar with the [documentation](https://albumentations.ai/docs/) of albumentations ([List of Tranforms](https://albumentations.ai/docs/getting_started/transforms_and_targets/)). Implement\n",
    "the augmentation pipeline for training and validation. Use the following augmentation for the training set:\n",
    "   * Crop a random part of the image and resize it to `IMAGE_SHAPE`. Crop minimum 60% of the image and maximum 100%. Use aspect rations between 0.75 and 1.33.\n",
    "   * Horizontal flip with probability 0.5\n",
    "   * With probability 0.9 **either** CLAHE **or** random brightness & contrast\n",
    "   * With probability 0.9 **either** random gamma **or** random hue, saturation and value\n",
    "\n",
    "\n",
    "The validation set should only be resized to `IMAGE_SHAPE`.\n",
    "\n",
    "Of cause normalize the image using `MEAN` and `STD` and convert it to a Pytorch tensor with `ToTensorV2()` at the end.\n",
    "If not specified use default parameters for the augmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "############################################################\n",
    "###                  START OF YOUR CODE                  ###\n",
    "############################################################\n",
    "train_transform = None\n",
    "test_transform = None\n",
    "############################################################\n",
    "###                   END OF YOUR CODE                   ###\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the augmentation pipeline is implemented we can start with the dataloader. To apply the albumantations pipeline to the\n",
    "torchvision dataset we need the helper `apply_transform`, that converts the PIL images to numpy arrays, applies the\n",
    "transformations and extracts the result. Have a look at the helper function and run the following cells to see some examples of your augmented images and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform(image, mask, train=True):\n",
    "    # choose either train or test transform\n",
    "    func = train_transform if train else test_transform\n",
    "    # convert image and mask von PIL.Image to numpy.ndarray and apply albumentations style transform\n",
    "    res = func(image=np.array(image), mask=np.array(mask))\n",
    "    # extract image and mask from the result\n",
    "    # subtract 1 from mask to make it 0-based\n",
    "    return res['image'], res['mask'] - 1\n",
    "\n",
    "\n",
    "train_ds = datasets.OxfordIIITPet(root=ROOT_PATH,\n",
    "                                  target_types='segmentation',\n",
    "                                  transforms=partial(apply_transform, train=True),\n",
    "                                  split='trainval')\n",
    "\n",
    "val_ds = datasets.OxfordIIITPet(root=ROOT_PATH,\n",
    "                                target_types='segmentation',\n",
    "                                transforms=partial(apply_transform, train=False),\n",
    "                                split='test')\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[43mtrain_loader\u001b[49m)\n\u001b[0;32m      2\u001b[0m images, masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_loader_iter)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImages have shape   : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader_iter = iter(train_loader)\n",
    "images, masks = next(train_loader_iter)\n",
    "\n",
    "print(f\"Images have shape   : {images.shape}\")\n",
    "print(f\"Masks have shape    : {masks.shape}\")\n",
    "\n",
    "# create inverse transform to visualize images\n",
    "inverse = A.Compose([\n",
    "    A.Normalize(mean=[-m / s for m, s in zip(MEAN, STD)], std=[1 / s for s in STD], max_pixel_value=1.0)\n",
    "])\n",
    "\n",
    "for i in range(3):\n",
    "    img = images[i].permute(1, 2, 0)  # swap channels\n",
    "    img = inverse(image=img.numpy())['image']\n",
    "    plot_segmentation(img, masks[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 2 Model (20 Points)\n",
    "\n",
    "---\n",
    "\n",
    "## U-NET\n",
    "---\n",
    "[U-Net](https://arxiv.org/pdf/1505.04597.pdf) is a deep learning architecture that is commonly used for image segmentation tasks. It was first proposed in 2015 by researchers at the University of Freiburg in Germany, and it has since been widely used in various applications, such as medical imaging and satellite image analysis.\n",
    "\n",
    "U-Net is a fully convolutional network, which means that it is composed entirely of convolutional layers, with no fully-connected layers. This allows it to take in inputs of arbitrary size and produce output segmentation masks of the same size. The architecture of U-Net is designed to be highly symmetrical, with a \"down-sampling\" path that encodes the input image into a compact representation, and a \"up-sampling\" path that decodes the compact representation back into a full-sized output mask.\n",
    "\n",
    "One of the key features of U-Net is its use of skip connections, which allows it to combine low-level details from the down-sampling path with high-level context from the up-sampling path. This helps the network to make more accurate segmentation predictions by leveraging both local and global information about the input image.\n",
    "\n",
    "\n",
    "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" alt=\"U-Net architecture figure\" width=\"70%\">\n",
    "\n",
    "We will follow the idea of U-Net, but we will not implement exactly the same architecture as in the figure. Instead, we will use a pre-trained encoder and extend it with our own decoder. This will speed up the convergence of the network and reduce the amount of computational resources. We use the same `regnet_x_400mf` pretrained on ImageNet as in the previous task, but this time we take features from five positions. Execute the following cell to print the output shapes of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import Encoder\n",
    "\n",
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can implement the decoder. For this purpose, a DecoderBlock is defined that receives both the skip connection from the encoder and the bottom-up stream and merges them accordingly by upsampling.\n",
    "**TODO**: Follow the instructions in `unet.py` and implement all methods in `DecoderBlock()` and `Decoder()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import UNet\n",
    "\n",
    "model = UNet(num_classes=10)\n",
    "x = torch.rand(2, 3, 224, 224)\n",
    "y = model(x)\n",
    "\n",
    "print(f\"[Test Model]: {tuple(y.shape) == (2, 10, 224, 224)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 3 Loss (20 Points)\n",
    "\n",
    "---\n",
    "\n",
    "## Dice Loss\n",
    "---\n",
    "We want to implement the multi-class **Dice Loss**, a popular loss function from the field of medical image segmentation. It measures the overlap between the predicted and the ground truth sample.\n",
    "\n",
    "For classes $c \\in C$ and samples $i \\in I$: $y_i^{(c)}$ is the one-hot encoding (0 or 1) of class $c$ and $\\hat{y}_i^{(c)}$ is the probability for the example and class. The Dice Loss defined as:\n",
    "\n",
    "$$\n",
    "    L(y, \\hat{y}) = 1 - \\frac{2}{|C|} \\cdot \\sum_{c \\in C} \\frac{\\sum_{i} y_i^{(c)} \\hat{y}_i^{(c)} + \\epsilon}{\\sum_{i} \\left(y_i^{(c)} + \\hat{y}_i^{(c)} \\right) + \\epsilon}\n",
    "$$\n",
    "where $\\epsilon$ is a constant to avoid dividing by zero.\n",
    "\n",
    "---\n",
    "\n",
    "**TODO**: Implement the forward pass of the class `DiceLoss` in `loss.py` and test your implementation in the cell below. It expects the logits of the model, so you must apply softmax to get probabilities. For numerical stability, use log_softmax and then exponentiate. The target is expected to be in one-hot format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import DiceLoss\n",
    "\n",
    "# model output of shape (N, C, H, W)\n",
    "logits = torch.tensor([[[[0.1261, 0.4626, 1.5762],\n",
    "                         [1.5561, -0.0415, -0.0891],\n",
    "                         [0.7072, 0.1905, -0.8896]],\n",
    "\n",
    "                        [[-1.4516, -0.6939, -0.2739],\n",
    "                         [1.5394, 1.1466, 0.7905],\n",
    "                         [-3.0648, -1.0307, 0.7960]]]])\n",
    "# one-hot ground truth of shape (N, C, H, W)\n",
    "target = torch.tensor([[[[1, 1, 0],\n",
    "                         [1, 0, 1],\n",
    "                         [1, 1, 0]],\n",
    "\n",
    "                        [[0, 0, 1],\n",
    "                         [0, 1, 0],\n",
    "                         [0, 0, 1]]]])\n",
    "\n",
    "expected_loss = torch.tensor(0.37266626954078674)\n",
    "\n",
    "dice = DiceLoss(reduce='mean')\n",
    "loss = dice(logits, target)\n",
    "passed = abs(loss - expected_loss) < 1e-5\n",
    "\n",
    "print(f\"[Test Loss]: {passed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 4 Metric (20 Points)\n",
    "\n",
    "---\n",
    "The performance of the segmentation will be evaluated with the **Intersection over Union** (Jaccard Index) and **Accuracy**. These metrics can be formulated with the results of a confusion matrix:\n",
    "$$\n",
    "    \\text{IoU} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN} + \\text{FP}}\n",
    "$$\n",
    "$$\n",
    "    \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN}+ \\text{FN} + \\text{FP}}\n",
    "$$\n",
    "with TP=True positives, TN=True negatives, FP=False positives and FN=False negatives.\n",
    "\n",
    "These values should be calculated pixel-wise for every class, to get the overall scores for the image.\n",
    "\n",
    "---\n",
    "\n",
    "**TODO**: Implement the functions `stat_scores()` to calculate the values of the confusion matrix as well as `accuracy_score()` and `iou_score()` in `metric.py` and test your implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(730) tensor(1690) tensor(555) tensor(13409)\n",
      "[Test Stats]: False\n",
      "[Test IoU]: False\n",
      "[Test Accuracy]: False\n"
     ]
    }
   ],
   "source": [
    "from metrics import stat_scores, iou_score, accuracy_score\n",
    "\n",
    "# create some dummy input with three classes\n",
    "test_img1 = torch.zeros(128, 128)\n",
    "test_img1[25:50, 25:50] = 1\n",
    "test_img1[100:120, 40:73] = 2\n",
    "\n",
    "test_img2 = torch.zeros(128, 128)\n",
    "test_img2[25:50, 25:45] = 1\n",
    "test_img2[80:110, 50:114] = 2\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(test_img1)\n",
    "ax[1].imshow(test_img2)\n",
    "plt.show()\n",
    "\n",
    "expected_iou, expected_acc = 0.7589779496192932, 0.9086506962776184\n",
    "expected_stats = torch.tensor([14139, 2245, 2245, 30523])\n",
    "\n",
    "tp, fp, fn, tn = stat_scores(test_img1, test_img2, num_classes=3)\n",
    "print(tp, fp, fn, tn)\n",
    "print(f\"[Test Stats]: {torch.allclose(torch.stack([tp, fp, fn, tn]), expected_stats)}\")\n",
    "\n",
    "iou = iou_score(tp, fp, fn, tn)\n",
    "print(f\"[Test IoU]: {abs(iou - expected_iou) < 1e-5}\")\n",
    "\n",
    "acc = accuracy_score(tp, fp, fn, tn)\n",
    "print(f\"[Test Accuracy]: {abs(acc - expected_acc) < 1e-5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Accuracy (5 Points)\n",
    "**TODO**: The accuracy score is rarely used in semantic segmentation because it can lead to misleading results. Briefly explain in your own words what the problem with accuracy is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 5 Training (20 Points)\n",
    "\n",
    "At this point, only three small things are missing to train the model. 1. The ground truth masks from the data loader have the shape `(N, H, W)`, so an integer represents the class label. But our loss function expects one-hot encoded targets. 2. For the evaluation with our created metrics we have to convert the output of the network into probabilities. 3. We need to calculate our metrics during training.\n",
    "\n",
    "---\n",
    "\n",
    "**TODO**: Implement two missing parts in `Solver.test()`, `Solver.infer()` and `Solver.train()` in `solver.py` and train the network for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solver import Solver\n",
    "from loss import DiceLoss\n",
    "from unet import UNet\n",
    "\n",
    "loss = DiceLoss(reduce='mean')\n",
    "model = UNet(num_classes=NUM_CLASSES)\n",
    "\n",
    "solver = Solver(train_loader, val_loader, model, loss, DEVICE)\n",
    "hist = solver.train(num_epochs=10)\n",
    "\n",
    "plot_history(hist, save=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected IoU-Score should be between 80-90%. Let's look at some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks = next(iter(val_loader))\n",
    "pred_masks = solver.infer(images)\n",
    "\n",
    "for image, mask, pred_mask in zip(images, masks, pred_masks):\n",
    "    image = image.permute(1, 2, 0)\n",
    "    image = inverse(image=image.numpy())['image']\n",
    "    plot_segmentation(image, mask, pred_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 6 Mask RCNN (10 Points)\n",
    "\n",
    "---\n",
    "\n",
    "In the last exercise we implemented the two-stage object detector Faster R-CNN. In the lecture, [Mask R-CNN](https://arxiv.org/pdf/1703.06870.pdf) was presented, that extends this approach to accomplish the segmentation task.\n",
    " **TODO:** Briefly describe the stages of Mask R-CNN (RPN -> Fast R-CNN) and explain how the segmentation is performed. Also describe why *RoIAlign* is used in this context and what problem it solves compared to *RoiPool*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "*Write your answer here.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
