{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfb5226",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "**Goethe University Frankfurt am Main**\n",
    "\n",
    "Winter Semester 2022/23\n",
    "\n",
    "<br>\n",
    "\n",
    "## *Assignment 9 (Generative Adversarial Networks)*\n",
    "\n",
    "---\n",
    "\n",
    "**Points:** 110<br>\n",
    "**Due:** 15.2.2023, 10 am<br>\n",
    "**Contact:** Matthias Fulde ([fulde@cs.uni-frankfurt.de](mailto:fulde@cs.uni-frankfurt.de))<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Your Name:** Tilo-Lars Flasche\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4017d8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "---\n",
    "\n",
    "- [1 Networks](#1-Networks-(50-Points))\n",
    "  - [1.1 Discriminator](#1.1-Discriminator-(20-Points))\n",
    "  - [1.2 Generator](#1.2-Generator-(25-Points))\n",
    "  - [1.3 Initialization](#1.3-Initialization-(5-Points))\n",
    "- [2 Training](#2-Training-(40-Points))\n",
    "  - [2.1 Discriminator Loss](#2.1-Discriminator-Loss-(10-Points))\n",
    "  - [2.2 Generator Loss](#2.2-Generator-Loss-(5-Points))\n",
    "  - [2.3 Gradients](#2.3-Gradients-(5-Points))\n",
    "  - [2.4 Game](#2.4-Game-(20-Points))\n",
    "- [3 Image Generation](#3-Image-Generation-(20-Points))\n",
    "  - [3.1 Hyperparameter Tuning](#3.1-Hyperparameter-Tuning-(10-Points))\n",
    "  - [3.2 Improvements](#3.2-Improvements-(10-Points))\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f0c24",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we're working with PyTorch again. For training a generative adversarial network, we definitely need a GPU, so make sure that you're training on the correct device. You can use the statements below to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46963cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071ac527",
   "metadata": {},
   "source": [
    "We import some utility functions to count the number of parameters in a model and to show the training losses of the generator and discriminator networks in a plot. Furthermore we import classes for the generator and discriminator network and for handling the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19816511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import count_params, show_loss\n",
    "\n",
    "from networks import Discriminator, Generator\n",
    "from training import Game\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aa1deb-0ce0-4cf4-849c-a179eca5d357",
   "metadata": {},
   "source": [
    "### TODO remove the following cells until the 'Dataset' cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c74fddd-b90c-46a2-9356-909b4a1fb4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(latent_dim=100, channels=1, depth=4)\n",
    "z = torch.ones(16, 100, 1, 1)\n",
    "z.shape\n",
    "G(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f776367a-faac-4418-ba28-8e11743f6d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator(channels=1, depth=4)\n",
    "x = torch.ones(16, 1, 28, 28)\n",
    "D(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec10b3-67b4-4d5f-bfd3-ab240b11d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "D(G(z)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf458a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Dataset\n",
    "\n",
    "---\n",
    "\n",
    "We're going to use the MNIST dataset again for the image generating task in order to reduce the necessary model complexity and runtime of the algorithm. However, if you want something more difficult, feel free to load another image dataset from Torchvision!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905a766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use a larger batch size with this dataset.\n",
    "batch_size = 128\n",
    "\n",
    "# Cast to tensor and normalize.\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# We just merge the training and test sets to get more data.\n",
    "dataset = torch.utils.data.ConcatDataset([\n",
    "    torchvision.datasets.MNIST(\n",
    "        root='./datasets',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    ),\n",
    "    torchvision.datasets.MNIST(\n",
    "        root='./datasets',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "])\n",
    "\n",
    "# Make data loader for whole dataset.\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66790aa9",
   "metadata": {},
   "source": [
    "Note that the given batch size above is just a suggestion. No matter whether you use the MNIST dataset or something else, you can adjust this hyperparameter to see if you get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410390bf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Exercises\n",
    "\n",
    "---\n",
    "\n",
    "Our goal in this assignment is to create a more or less vanilla convolutional GAN to generate images similar to those from the MNIST dataset (or the dataset of your choice). In order to do this, we have to implement both the generator and discriminator networks, the loss functions, and the training algorithm.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1 Networks (50 Points)\n",
    "\n",
    "---\n",
    "\n",
    "The first step is to create the two networks and implement the weight initialization.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1.1 Discriminator (20 Points)\n",
    "\n",
    "---\n",
    "\n",
    "The task is to complete the definition of the `Discriminator` class in the `networks.py` file.\n",
    "\n",
    "The discriminator network is basically just a binary classifier. You're generally free to come up with a convolutional architecture on your own, with some restrictions. Your network should be fully convolutional without max pooling or linear layers, except for the output layer, where you're allowed to use a linear layer for the final projection.\n",
    "\n",
    "Furthermore, your discriminator network should incrementally decrease the size of the feature maps. For the MNIST dataset, intermediate sizes for the feature maps of $28, 14, 7$ should be sufficient. Downsample the feature maps by using appropriate values for kernel size, padding, and stride.\n",
    "\n",
    "You should use a sigmoid function for the final activation, mapping predictions in the $(0, 1)$ range. For the hidden layer activations it is recommended to use leaky ReLU. Other non saturating activation functions that do not produce sparse gradients can also work, though.\n",
    "\n",
    "Normalize the activations of each layer, except for the first and last layer. If you use a larger batch size, such as the default value given above, using regular spatial batch norm should be fine. If you monitor the generated images during training and find that they become correlated, try using virtual batch norm or instance normalization instead.\n",
    "\n",
    "There are no restrictions regarding the size of your model, but keep in mind that training a larger model takes more time. So it's recommended to start small and increase the capacity only if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dacbc5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.2 Generator (25 Points)\n",
    "\n",
    "---\n",
    "\n",
    "The task is to complete the definition of the `Generator` class in the `networks.py` file.\n",
    "\n",
    "The generator network should map latent vectors of some given dimension to image tensors with the same shape and value range as the images from the dataset. You're again free to come up with a convolutional architecture on your own, with some restrictions. As with the discriminator, your network should be fully convolutional except for the very first layer, where you're allowed to use a linear layer for the initial projection of the latent vectors.\n",
    "\n",
    "Furthermore, your generator should incrementally increase the size of the feature maps. For the MNIST dataset, intermediate sizes for the feature maps of $7, 14, 28$ should be sufficient. Use either transposed convolution or the pixel shuffle operator for upsampling the feature maps.\n",
    "\n",
    "For transposed convolution you can compute the output dimensions as:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    D_\\text{out} = (D_\\text{in} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{kernel_size}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "You should use a tanh function for the final activation of the network, mapping pixel values in the $(-1, 1)$ range. For the hidden layer activations it is again recommended to use the leaky ReLU function.\n",
    "\n",
    "As with the discriminator network, you should normalize the activations of each layer, except for the first and last layer of the network. Regular batch norm should be fine. If you encounter issues, use virtual batch norm or instance normalization as already noted above.\n",
    "\n",
    "Again, there is no restriction regarding the number of parameters in your model, but it is recommended to start small and increase the size only if necessary. The network sizes do not have to match, but the differences in size shouldn't become too large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9643d170",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.3 Initialization (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "The task is to complete the definition of the `init_params` function in the `networks.py` file.\n",
    "\n",
    "We want to initialize the parameters of the convolution, transposed convolution, and normalization layers in a particular way. The function is a callback that is recursively called with each submodule of your generator and discriminator networks. You should check whether the argument is a convolutional or normalization layer and if so, initialize the parameters as follows:\n",
    "\n",
    "- `Conv2d` or `ConvTranspose2d`: Initialize the weights with values drawn from $\\mathcal{N}(0, 0.02)$\n",
    "- `Norm2d`: Initialize the weights with values drawn from $\\mathcal{N}(1, 0.02)$ and the biases with constant $0$\n",
    "\n",
    "Note that you shouldn't use biases for the convolutional layers when using normalization. Remember that you've shown in a previous assignment that the biases are redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e04a79f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2 Training (40 Points)\n",
    "\n",
    "---\n",
    "\n",
    "The next step is to create the training routine for the GAN game.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.1 Discriminator Loss (10 Points)\n",
    "\n",
    "---\n",
    "\n",
    "The task is to complete the definition of the `discriminator_loss` function in the `training.py` file.\n",
    "\n",
    "Implement the discriminator loss as introduced in the lecture:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    J^{(D)} =\n",
    "    -\\frac{1}{2}\\mathbb{E}_{\\mathbf{x} \\sim p_\\text{data}(\\mathbf{x})} \\ln D(\\mathbf{x})\n",
    "    -\\frac{1}{2}\\mathbb{E}_{\\mathbf{z} \\sim p(\\mathbf{z})} \\ln\\left(1 - D(G(\\mathbf{z}))\\right)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The function takes as input the predictions for a minibatch of samples from the training set and a minibatch of samples created by the generator network and returns the sum of the means of the respective losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702bbcf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.2 Generator Loss (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "The task is to complete the definition of the `generator_loss` function in the `training.py` file.\n",
    "\n",
    "Implement the *non-saturating* loss as introduced in the lecture:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    J^{(G)} = -\\frac{1}{2}\\mathbb{E}_{\\mathbf{z} \\sim p(\\mathbf{z})}\\ln D(G(\\mathbf{z}))\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Recall that this is different from the generator loss in the zero-sum game. We use this version to have a stronger gradient signal when the generated images are bad, so that the generator can better learn from its errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f9b5f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.3 Gradients (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "The task is to complete the definition of the `requires_grad` function in the `training.py` file.\n",
    "\n",
    "The generator and discriminator networks are trained jointly, in alternating runs. When training the discriminator, we also need to employ the generator, and vice versa. In order to avoid unnecessary computations, we will set the `requires_grad` attribute of parameters of the network that is currently not optimized to `False`.\n",
    "\n",
    "The function should iterate over the parameters of the given model and set the attributes accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996012e1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.4 Game (20 Points)\n",
    "\n",
    "---\n",
    "\n",
    "The task is to complete the definition of the `Game` class in the `training.py` file.\n",
    "\n",
    "This class implements the GAN game. Check the parameters and what attributes are stored in the constructor. The class provides methods for training, saving and loading state, and creating images for visual inspection during training.\n",
    "\n",
    "Your task is to implement the main training loop in the `play` method of the class. We use the function `iterate` to sample an arbitrary amount of random minibatches during training, specified by the `num_iter` parameter of the method. In each iteration you should do the following:\n",
    "\n",
    "- **Discriminator training**\n",
    "  - Use the `requires_grad` function to set the corresponding attributes on the networks.\n",
    "  - Sample a minibatch of random vectors of size `latent_dim` from $\\mathcal{N}(0,1)$\n",
    "  - Generate fake images.\n",
    "  - Compute predictions for real and fake images.\n",
    "  - Compute discriminator loss and append the value to the `self.d_loss` list.\n",
    "  - Update discriminator paramerers.\n",
    "\n",
    "\n",
    "- **Generator training**\n",
    "  - Use the `requires_grad` function to set the corresponding attributes on the networks.\n",
    "  - Sample a minibatch of random vectors of size `latent_dim` from $\\mathcal{N}(0,1)$\n",
    "  - Generate fake images.\n",
    "  - Compute prediction for fake images.\n",
    "  - Compute generator loss and append the value to the `self.g_loss` list.\n",
    "  - Update generator parameters.\n",
    "\n",
    "Note that we use Adam optimizer, with $\\beta_1$ set to $0.5$, and a uniform learning rate for both networks. You may change that if you want to try out different algorithms, betas, or different learning rates for generator and discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8922dcf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3 Image Generation (20 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Now that we have everything implemented, let's train our model for some time to see how well it works!\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3.1 Hyperparameter Tuning (10 Points)\n",
    "\n",
    "---\n",
    "\n",
    "The task is to set up your model and training algorithm to get some good results. Finetune your hyperparameters to get a stable training run without convergence failure or mode collapse. An important part of training is visual inspection. The `Game` class saves grids of generated images in the `images` folder. Use these images to monitor the progress of your model and use early stopping if it's clear that the model doesn't converge.\n",
    "\n",
    "You can use the `prefix` parameter of the constructor to give a unique name to the saved images and the `show_every` parameter to set an interval for generating samples. After training, keep *only* the best result for this exercise in the `images` folder for your submission.\n",
    "\n",
    "The goal in this exercise is not to perfectly match $p_\\text{data}(\\mathbf{x})$. It's sufficient if your generator produces reasonably good results that are clearly recognizable as being close to samples the target distribution. See the image below for an acceptable result for the MNIST dataset.\n",
    "\n",
    "<br>\n",
    "\n",
    "![Example outputs](images/example.jpg)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3.1.1 Solution\n",
    "\n",
    "Adjust the code below to your needs for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8514ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "###                  START OF YOUR CODE                  ###\n",
    "############################################################\n",
    "\n",
    "# Set latent dimension and number of channels.\n",
    "latent_dim = 100\n",
    "channels = 1\n",
    "depth = 4\n",
    "\n",
    "# Create new training setup.\n",
    "game = Game(\n",
    "    Discriminator(channels, depth),\n",
    "    Generator(latent_dim, channels, depth),\n",
    "    dataloader,\n",
    "    device,\n",
    "    batch_size=batch_size,\n",
    "    latent_dim=latent_dim,\n",
    "\n",
    "    # kwargs ...\n",
    ")\n",
    "\n",
    "############################################################\n",
    "###                   END OF YOUR CODE                   ###\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9307f996",
   "metadata": {},
   "source": [
    "Note that the `Game` class saves the training state. You can execute the following cell with the call to the the `play` method multiple times and don't have to train your model in a single run. Also note that you can use the `save` and `load` methods of the class to save the training state to disc and continue training at a later time. You don't have to upload your model for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb2fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = game.play(num_iter=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a0e3e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.1.2 Results\n",
    "\n",
    "Use the function call below to show the accumulated training loss history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7875c1eb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3.2 Improvements (10 Points)\n",
    "\n",
    "---\n",
    "\n",
    "The task is to implement at least *one* additional feature to the training algorithm that could improve your previous results. For instance you can implement R1 regularization, one-sided label smoothing, a replay buffer, additional noise, or other techniques not used so far that were introduced in the lecture or that you find in literature (give references when you implement something not mentioned in the lecture).\n",
    "\n",
    "You're free to add additional parameters and methods to the `Game` class for your implementation, or additional functions in the `training.py` file.\n",
    "\n",
    "Again keep only your best result in the `images` folder for your submission. (Remember using a different prefix for this training run.)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3.2.1 Solution\n",
    "\n",
    "Adjust the code below to train your model again using the new implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "###                  START OF YOUR CODE                  ###\n",
    "############################################################\n",
    "\n",
    "# Set latent dimension and number of channels.\n",
    "latent_dim = None\n",
    "channels = 1\n",
    "depth = None\n",
    "\n",
    "# Create new training setup.\n",
    "game = Game(\n",
    "    Discriminator(channels, depth),\n",
    "    Generator(latent_dim, channels, depth),\n",
    "    dataloader,\n",
    "    device,\n",
    "    batch_size=batch_size,\n",
    "    latent_dim=latent_dim,\n",
    "\n",
    "    # kwargs ...\n",
    ")\n",
    "\n",
    "############################################################\n",
    "###                   END OF YOUR CODE                   ###\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacc6974",
   "metadata": {},
   "source": [
    "Train the networks for some time using the updated algorithm. For comparability, you should run the training for the same number of iterations as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e9c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = game.play(num_iter=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be454d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.2.2 Results\n",
    "\n",
    "Use the function call below to show the accumulated training loss history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9086869d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.2.3 Discussion\n",
    "\n",
    "Briefly describe what you have changed and how it did affect the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b2948",
   "metadata": {},
   "source": [
    "*Write your descriptions here.*\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff5799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
