{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfb5226",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "**Goethe University Frankfurt am Main**\n",
    "\n",
    "Winter Semester 2022/23\n",
    "\n",
    "<br>\n",
    "\n",
    "## *Assignment 3 (Network)*\n",
    "\n",
    "---\n",
    "\n",
    "**Points:** 25<br>\n",
    "**Due:** 16.11.2022, 10 am<br>\n",
    "**Contact:** Matthias Fulde ([fulde@cs.uni-frankfurt.de](mailto:fulde@cs.uni-frankfurt.de))<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Your Name:** Tilo-Lars Flasche\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cacdc3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "---\n",
    "\n",
    "- [1 Loss](#1-Loss)\n",
    "- [2 Optimization](#2-Optimization-(5-Points))\n",
    "  - [2.1 Gradient Descent with Momentum](#2.1-Gradient-Descent-with-Momentum-(3-Points))\n",
    "  - [2.2 Weight Decay](#2.2-Weight-Decay-(2-Points))\n",
    "- [3 Deep Neural Network](#3-Deep-Neural-Network-(20-Points))\n",
    "  - [3.1 Definition](#3.1-Definition-(5-Points))\n",
    "  - [3.2 Training](#3.2-Training-(15-Points))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51799a38",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "Besides the NumPy and Matplotlib libraries, we import the definitions of the network layers and the corresponding test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b791b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from modules import *\n",
    "from utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d690495",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Exercises\n",
    "\n",
    "---\n",
    "\n",
    "### 1 Loss\n",
    "\n",
    "---\n",
    "\n",
    "In this assignment we want to create and train a deep neural network for classification, hence we need a loss function. We want to use again the cross-entropy loss that we already implemented in the last assignment.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1.1 Task\n",
    "\n",
    "Complete the definition of the `CrossEntropyLoss` class in the `modules/loss.py` file.\n",
    "\n",
    "Feel free to use the implementation shown in the solution for last week's assignment, but you can also copy your own solution. This exercise is not graded. If you have not implemented this loss in the previous assignment, we highly recommend that you try this yourself before using the given solution.\n",
    "\n",
    "In the `forward` method of the class, compute the cross-entropy loss and store the result in the `out` variable that is returned from the method. Also cache the labels and the softmax probabilities to reuse them in the backward pass for gradient computation.\n",
    "\n",
    "In the `backward` method compute the gradient of the loss with respect to the inputs. Store the gradient in the `in_grad` variable that is passed to the given model and that is returned from the method.\n",
    "\n",
    "Use only vectorized operations.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1.2 Test\n",
    "\n",
    "To test the implementation you can run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5833de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CrossEntropyLoss_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb0e277",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2 Optimization (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "So far we have only used vanilla gradient descent. That is, the update rule was to scale the gradient with the learning rate and subtract the result from the parameters. In this exercise we want to extend that concept a bit, taking into account also the previous updates.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.1 Gradient Descent with Momentum (3 Points)\n",
    "\n",
    "---\n",
    "\n",
    "One of the problems with standard gradient descent is that the gradient with respect to a parameter may change rapidly during training. These oscillations of the gradient make optimization hard. In addition, there is also the problem of the gradient being stuck in a flat region, where the slope is almost zero. Gradient descent with momentum is one approach to tackle these problems.\n",
    "\n",
    "Instead of just updating the parameters with\n",
    "\n",
    "$$\n",
    "    W^{(t)} = W^{(t-1)} - \\eta\\nabla\\mathcal{L}\\left(W^{(t-1)}\\right)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where $\\eta$ is the learning rate, we also take into account the previous updates of the parameters, scaled by a hyperparameter called momentum. Thus the update rule becomes\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    W^{(t)} = W^{(t-1)} - V^{(t)}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "    V^{(t)} = \\mu V^{(t-1)} + \\eta\\nabla\\mathcal{L}\\left(W^{(t-1)}\\right)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where $V$ is called velocity and $\\mu$ is the momentum. The velocity is an array with the same shape as $W$ and $\\nabla\\mathcal{L}(W)$ and can be understood as a moving average over the past gradients. With this update rule we get a more stable trajectory towards a minimum and we can still move, even if the gradient for the current time step becomes small.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.1.1 Task\n",
    "\n",
    "Complete the definition of the `SGD` class in the `modules/optim.py` file.\n",
    "\n",
    "In the `step` method of the optimizer, implement the update rule described above. The $\\text{learning_rate}$ and $\\text{momentum}$ are stored as attributes of the optimizer object and each layer that is iterated over has dictionaries for parameters, gradients and velocity, where corresponding entries are referenced with the same name, given that you adhered to this convention in the previous exercises.\n",
    "\n",
    "Your implementation should be fully vectorized, so no loops are allowed.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.1.2 Test\n",
    "\n",
    "To test your implementation, run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa929f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c17f6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.2 Weight Decay (2 Points)\n",
    "\n",
    "---\n",
    "\n",
    "In the last assignment, we used L2 regularization to regularize our linear classifier models, computing the squared Euclidean norm of the parameters. We added explicitly a regularization loss term to the data loss term to compute the final loss. However, we can also implement this in a slightly different way.\n",
    "\n",
    "Instead of computing an explicit loss, we can apply **weight decay**, by just adding the gradient of the L2 regularization loss separately for each parameter to the gradient of the data loss. Hence, for vanilla gradient descent with weight decay we compute\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    W^{(t)} = W^{(t-1)} - \\eta\\left(\\nabla\\mathcal{L}\\left(W^{(t-1)}\\right) + \\lambda W^{(t-1)}\\right),\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where $\\eta$ is the learning rate and $\\lambda$ is the regularization strength. This is based on the equivalent definition of the regularization loss as\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    R(W) = \\frac{\\lambda}{2}\\lVert W \\rVert^2.\n",
    "$$\n",
    "\n",
    "such that\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial}{\\partial W} R(W) = \\lambda W.\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "In the same way we can compute the update when using stochastic gradient descent with momentum. In this case we compute\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    W^{(t)} = W^{(t-1)} - V^{(t)}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "    V^{(t)} = \\mu V^{(t-1)} + \\eta\\left(\\nabla\\mathcal{L}\\left(W^{(t-1)}\\right) + \\lambda W^{(t-1)}\\right).\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.2.1 Task\n",
    "\n",
    "Extend the definition of the `SGD` class in the `modules/optim.py` file.\n",
    "\n",
    "Add weight decay to the update rule of stochastic gradient descent with momentum, which you implemented in the previous exercise.\n",
    "\n",
    "Use only vectorized operations.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.2.2 Test\n",
    "\n",
    "To test your implementation, run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec16e4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD_test(use_weight_decay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19b99b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3 Deep Neural Network (20 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Now that we have all the components implemented, we can plug everything together to create a deep neural network.\n",
    "\n",
    "Since we don't have GPU support, we're going to create a rather shallow model. Otherwise the training would take too much time. In order to get an idea how many parameters our model will have in the end, we'll calculate them from hand after the definition.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3.1 Definition (5 Points)\n",
    "\n",
    "To test our implementations, we're going to define a network with two convolutional layers, each followed by a ReLU activation function and a max pooling layer. We convert the outputs of the last pooling layer into vectors and pass them into a small fully-connected network, composed of two linear layers, the first of which has a ReLU activation function. The last linear layer has no activation function and produces the scores for the ten classes of the dataset.\n",
    "\n",
    "For both convolutional layers we use a kernel size of 3 and set padding and stride to 1. The first conv layer has 3 input channels and 6 output channels. The second conv layer has 6 input channels and 8 output channels.\n",
    "\n",
    "For the pooling layers we use a kernel size of 2 and a stride of the same size, so that we pool non-overlapping windows of the feature maps.\n",
    "\n",
    "The number of output features for the first linear layer should be 32, and for the second linear layer it should be 10, matching the number of classes in the CIFAR-10 dataset that we use again in this exercise.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3.1.1 Feature Size (1 Point)\n",
    "\n",
    "The resolution of the images in the CIFAR-10 dataset is $32\\times32$. Given the definitions above, compute the number of input features for the first linear layer. Write down all the steps of your computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35bc47",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "*Write your answer here.*\n",
    "\n",
    "The first convolution layer receives images of shape 32x32 = 1024 and 3 channels, which means that this layer in total has 3x1024 = 3072 features. It uses a convolution kernel of shape 3x3 which means that the output feature maps would have a width and height of 32-3+1=30 pixels if we would not apply padding to the image. As we padding of size one to the image the convolution layer will produce feature maps of shape 32x32. As there are six channels produces by the convolution layer there are 32x32x6=6144 features calculated.\n",
    "\n",
    "The pooling layer reduces the shape of each channel by half in each dimension, so there are 6144 * 0.5^2 = 1536 features that the pooling layer will output. These features are spread equally over the six channels, so each channel will have 1536 / 6 = 256 features. Each feature map is of shape 16x16\n",
    "\n",
    "Now the six feature maps are passed through teh next convolutional layer. This layer will produce eight channels. Because this layer also pads each feature map with one pixel for each border the resulting feature maps will have 16x16 pixels. \n",
    "\n",
    "The eight feature maps are again passed through a pooling layer which again reduces the shape of the feature maps by half for each dimension. The resulting feature maps will be of shape 8x8. As we have eight feature maps the pooling layer outputs 8x8x8 = 512 features.\n",
    "\n",
    "These 512 features are then passed to the first linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3f83f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.1.2 Implementation (2 Points)\n",
    "\n",
    "Complete the definition of the `ConvNet` class below.\n",
    "\n",
    "If you don't define the `forward` method, the inherited method from the base class will call the layers in the order in which they were added as attributes in the constructor. You don't have to define a `backward` method.\n",
    "\n",
    "Create the network according to the above definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6484635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Create deep neural network with two conv and two linear layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ############################################################\n",
    "        ###                  START OF YOUR CODE                  ###\n",
    "        ############################################################\n",
    "\n",
    "        # First block of convolution, activation and pooling layers\n",
    "        self.convolution1 = Conv2d(in_channels=3, out_channels=6, kernel_size=3, padding=1, stride=1)\n",
    "        self.activation1 = ReLU()\n",
    "        self.pooling1 = MaxPool(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second block of convolution, activation and pooling layers\n",
    "        self.convolution2 = Conv2d(in_channels=6, out_channels=8, kernel_size=3, padding=1, stride=1)\n",
    "        self.activation2 = ReLU()\n",
    "        self.pooling2 = MaxPool(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.vectorize = Vector()\n",
    "        self.linear1 = Linear(in_features=512, out_features=32)\n",
    "        self.activation3= ReLU()\n",
    "        self.linear2 = Linear(in_features=32, out_features=10)\n",
    "\n",
    "        ############################################################\n",
    "        ###                   END OF YOUR CODE                   ###\n",
    "        ############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3527ee",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.1.3 Capacity (2 Points)\n",
    "\n",
    "Now we want to compute the capacity of the model, which is the number of learnable parameters. Compute the number of parameters for each layer and than sum the results to get the total number of parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3ab07d",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "*Write your answer here.*\n",
    "\n",
    "| layer        | params |\n",
    "|--------------|--------|\n",
    "| convolution1 | (3x3x3+1)x6 = 168     |\n",
    "| pooling1     | 0      |\n",
    "| activation1  | 0      |\n",
    "| convolution2 | (3x3x3+1)x8 = 224     |\n",
    "| pooling2     | 0      |\n",
    "| activation2  | 0      |\n",
    "| linear1  | (512+1)x32 = 16416      |\n",
    "| activation3  | 0      |\n",
    "| linear2  | (32+1)x10 = 330      |\n",
    "\n",
    "The convolutional layers have 3x3x3+1 parameters per channel. 3x3x3 is the kernel size plus one for the bias.\n",
    "\n",
    "The pooling layer has no parameters and so does the activation layer.\n",
    "\n",
    "The first linear layer takes 512 features plus the bias and is fully connected to the next layer which has 32 features. So the linear layer has (512+1)x32 parameters.\n",
    "\n",
    "The final linear layer maps 32 features plus the bias to the ten output categories. This layer has (32+1)x10 parameters.\n",
    "\n",
    "In total the model has 17138 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d58d8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3.2 Training (15 Points)\n",
    "\n",
    "---\n",
    "\n",
    "We want to train our model again on the CIFAR-10 dataset that we already used in the previous problem sets. The function for loading and preprocessing the data expects the dataset in the `datasets` folder in the same directory as the notebook, so copy the folder before you proceed.\n",
    "\n",
    "Let's load the data and print the shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98b3659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (49000, 3, 32, 32)\n",
      "y_train shape: (49000,)\n",
      "X_test shape: (1000, 3, 32, 32)\n",
      "y_test shape: (1000,)\n",
      "X_val shape: (1000, 3, 32, 32)\n",
      "y_val shape: (1000,)\n",
      "X_dev shape: (500, 3, 32, 32)\n",
      "y_dev shape: (500,)\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the CIFAR-10 dataset.\n",
    "data = get_CIFAR_10_data()\n",
    "\n",
    "# Output the shapes of the partitioned data and labels.\n",
    "for name, array in data.items():\n",
    "    print(f'{name} shape: {array.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e2f57b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.2.1 Task (12 Points)\n",
    "\n",
    "Implement a training loop for the defined model and dataset.\n",
    "\n",
    "In each iteration, randomly sample a minibatch of $64$ images from the development set with replacement. Compute the forward pass through the network. In order to do this, you can call the model directly. The `__call__` method dispatches to the `forward` method of the instance.\n",
    "\n",
    "Compute the average accuracy for the training batch and store it in the predefined `train_acc` list.\n",
    "\n",
    "The next step is to call the loss function with the model output and the ground truth labels. Again, you can call the instance directly. Store the loss in the `train_loss` list that is already defined. After that, call the `backward` method of the loss to compute the gradients of the loss with respect to the model parameters.\n",
    "\n",
    "In order to update the parameters, call the `step` method of the optimizer.\n",
    "\n",
    "Finally, sample a minibatch of the same size from the validation set and compute a forward pass. Again compute the loss. Store it in the `val_loss` list. Compute the average accuracy of the predictions and store the result in the `val_acc` list.\n",
    "\n",
    "Use only vectorized operations. No further loops are allowed.\n",
    "\n",
    "<br>\n",
    "\n",
    "You're model should at least converge, so the loss should decrease and the accuracy increase. With the given settings, expect a slow start, but towards the end of the given number of iterations, you should see that the accuracy on the development set is well above chance, which would be $10\\%$.\n",
    "\n",
    "Since we're training only on the CPU, be prepared that training the model with the predefined settings may take a while!\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3.2.2 Solution\n",
    "\n",
    "Write your solution in the marked code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f951b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model.\n",
    "model = ConvNet()\n",
    "\n",
    "# Create the loss function.\n",
    "loss = CrossEntropyLoss(model)\n",
    "\n",
    "# Create the optimizer.\n",
    "optimizer = SGD(model, lr=1e-3, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Access development set.\n",
    "X_dev = data['X_dev']\n",
    "y_dev = data['y_dev']\n",
    "\n",
    "# Access validation set.\n",
    "X_val = data['X_val']\n",
    "y_val = data['y_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3f268bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store training and validation loss.\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# Lists to store the training and validation accuracy.\n",
    "train_acc = []\n",
    "val_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897ed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:    1  |  Train acc:   7.81%  |  Val acc:  14.06%  |  Train loss: 10.569  |  Val loss:  3.122\n",
      "Iter:   10  |  Train acc:  10.94%  |  Val acc:  12.50%  |  Train loss:  2.239  |  Val loss:  2.339\n",
      "Iter:   20  |  Train acc:  15.62%  |  Val acc:   9.38%  |  Train loss:  2.276  |  Val loss:  2.324\n",
      "Iter:   30  |  Train acc:  15.62%  |  Val acc:   7.81%  |  Train loss:  2.265  |  Val loss:  2.345\n",
      "Iter:   40  |  Train acc:  15.62%  |  Val acc:  12.50%  |  Train loss:  2.283  |  Val loss:  2.278\n"
     ]
    }
   ],
   "source": [
    "# Set number of iterations.\n",
    "num_iter = 100\n",
    "\n",
    "# Set number of samples per minibatch.\n",
    "batch_size = 64\n",
    "\n",
    "# Show intermediate results.\n",
    "verbose = True\n",
    "print_every = 10\n",
    "\n",
    "# Train the model.\n",
    "for i in range(1, 1+num_iter):\n",
    "    ############################################################\n",
    "    ###                  START OF YOUR CODE                  ###\n",
    "    ############################################################\n",
    "    \n",
    "    training, validation = [\n",
    "        (X_dev, y_dev, train_acc, train_loss, False),\n",
    "        (X_val, y_val, val_acc, val_loss, True)\n",
    "    ]\n",
    "    \n",
    "    # Iterate over training and validation batches\n",
    "    for (X, y, batch_acc, batch_loss, freeze) in [training, validation]:\n",
    "        # Sample mini batch\n",
    "        rand_idx = np.random.choice(X_dev.shape[0], batch_size, replace=True)\n",
    "        X_batch = X[rand_idx]\n",
    "        y_batch = y[rand_idx]\n",
    "\n",
    "        # Pass batch through the model\n",
    "        result = model(X_batch)\n",
    "    \n",
    "        # Compute predictions of the model and the accuracy\n",
    "        predictions = np.argmax(result, axis=1)\n",
    "        accuracy = (y_batch == predictions).sum() / len(predictions)\n",
    "    \n",
    "        # Store accuracy\n",
    "        batch_acc.append(accuracy)\n",
    "\n",
    "        # Compute and store loss\n",
    "        batch_loss.append(loss.forward(result, y_batch))\n",
    "\n",
    "        if not freeze:\n",
    "            # call the backward method of the loss to compute the gradients of the loss\n",
    "            grad = loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "    ############################################################\n",
    "    ###                   END OF YOUR CODE                   ###\n",
    "    ############################################################\n",
    "    if verbose and (i == 1 or i % print_every == 0):\n",
    "        print(\n",
    "            f'Iter: {i:4}  | ',\n",
    "            f'Train acc: {train_acc[-1]*100:6.2f}%  | ',\n",
    "            f'Val acc: {val_acc[-1]*100:6.2f}%  | ',\n",
    "            f'Train loss: {train_loss[-1]:6.3f}  | ',\n",
    "            f'Val loss: {val_loss[-1]:6.3f}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e0973",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.2.3 Results\n",
    "\n",
    "Let's check the best accuracy on the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42281f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best train acc: {np.max(train_acc)*100:6.2f}  |  Best val acc: {np.max(val_acc)*100:6.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a39e83",
   "metadata": {},
   "source": [
    "Let's also plot the training and validation losses and accuracies obtained during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training(train_loss, val_loss, train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80991241",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.2.4 Observations (3 Points)\n",
    "\n",
    "Briefly describe your observations when you trained the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d87b0e",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a8bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
