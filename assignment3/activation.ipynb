{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfb5226",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "**Goethe University Frankfurt am Main**\n",
    "\n",
    "Winter Semester 2022/23\n",
    "\n",
    "<br>\n",
    "\n",
    "## *Assignment 3 (Activation)*\n",
    "\n",
    "---\n",
    "\n",
    "**Points:** 15<br>\n",
    "**Due:** 16.11.2022, 10 am<br>\n",
    "**Contact:** Matthias Fulde ([fulde@cs.uni-frankfurt.de](mailto:fulde@cs.uni-frankfurt.de))<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Your Name:** Tilo-Lars Flasche\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b380cd99",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "---\n",
    "\n",
    "- [1 Rectified Linear Unit](#1-Rectified-Linear-Unit-(2-Points))\n",
    "- [2 Sigmoid](#2-Sigmoid-(7-Points))\n",
    "  - [2.1 Derivative](#2.1-Derivative-(5-Points))\n",
    "  - [2.2 Implementation](#2.2-Implementation-(2-Points))\n",
    "- [3 Hyperbolic Tangent](#3-Hyperbolic-Tangent-(1-Point))\n",
    "- [4 Saturating Nonlinearities](#4-Saturating-Nonlinearities-(5-Points))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbfa5ca",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "Besides the NumPy and Matplotlib libraries, we import the definitions of the activation functions and the corresponding test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e3265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from modules.activation import *\n",
    "from modules.activation_test import *\n",
    "\n",
    "from utils import show_activation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513be0bf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Exercises\n",
    "\n",
    "---\n",
    "\n",
    "### 1 Rectified Linear Unit (2 Points)\n",
    "\n",
    "---\n",
    "\n",
    "The rectified linear unit (ReLU) is a non-linear activation function commonly used in neural networks. It is defined as the positive part of its argument, so for some $x \\in \\mathbb{R}$ we compute\n",
    "\n",
    "$$\n",
    "    \\text{ReLU}(x) = \\max(0, x).\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The ReLU function is not differentiable everywhere, since the derivative at zero doesn't exist. However, we can just set the derivative to zero in this case. Hence, for the derivative of ReLU, we compute\n",
    "\n",
    "$$\n",
    "    \\frac{\\text{d}}{\\text{d}x}\\text{ReLU}(x)\n",
    "    =\n",
    "    \\begin{cases}\n",
    "        1 & \\text{if} \\: x >    0 \\\\\n",
    "        0 & \\text{if} \\: x \\leq 0.\n",
    "    \\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d7d05",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.1 Implementation\n",
    "\n",
    "---\n",
    "\n",
    "Complete the definition of the `ReLU` class in the `modules/activation.py` file.\n",
    "\n",
    "In the `forward` method, store the received inputs for gradient computation in the backward pass. Apply the ReLU activation function to the input componentwise and store the result in the `out` variable that is returned from the method.\n",
    "\n",
    "In the `backward` method, compute the gradient of the loss with respect to the inputs. The method receives the gradient of the loss with respect to the layer outputs. Store the gradient of the loss with respect to the layer inputs in the `in_grad` variable that is returned from the method.\n",
    "\n",
    "Your implementation should be fully vectorized, that is, no loops are allowed.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1.1.1 Test\n",
    "\n",
    "To test your implementation you can run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b95eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test case.\n",
    "ReLU_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6906ab7",
   "metadata": {},
   "source": [
    "#### 1.1.2 Graph\n",
    "\n",
    "Let's also have a look at the graph of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05146e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create activation.\n",
    "relu = ReLU()\n",
    "\n",
    "# Plot function for some values.\n",
    "show_activation(relu, val_range=(-5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1778423f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2 Sigmoid (7 Points)\n",
    "\n",
    "---\n",
    "\n",
    "The sigmoid activation function is defined as $\\sigma: \\mathbb{R} \\to \\mathbb{R}$ with\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    \\sigma(x) = \\frac{1}{1 + e^{-x}}.\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "It has the property that the output is in the range $(0,1)$, such that it can be interpreted as a probability.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.1 Derivative (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Show that the derivative of the sigmoid function is given by\n",
    "\n",
    "$$\n",
    "    \\frac{\\text{d}}{\\text{d}x}\\sigma(x) = \\sigma(x)(1-\\sigma(x).\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af125ef3",
   "metadata": {},
   "source": [
    "##### Proof\n",
    "\n",
    "*Write your proof here.*\n",
    "\n",
    "<div style=\"text-align:right\">$\\square$</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de992cad",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.2 Implementation (2 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Complete the definition of the `Sigmoid` class in the `modules/activation.py` file.\n",
    "\n",
    "In the `forward` method, apply the sigmoid activation function to the input componentwise and store the result in the `out` variable that is returned from the method. Save the computed outputs for gradient computation in the backward pass.\n",
    "\n",
    "In the `backward` method, compute the gradient of the loss with respect to the inputs. The method receives the gradient of the loss with respect to the layer outputs. Store the gradient of the loss with respect to the layer inputs in the `in_grad` variable that is returned from the method.\n",
    "\n",
    "Your implementation should be fully vectorized, that is, no loops are allowed.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.2.1 Test\n",
    "\n",
    "To test your implementation you can run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e07e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigmoid_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a0378",
   "metadata": {},
   "source": [
    "#### 2.2.2 Graph\n",
    "\n",
    "Let's also have a look at the graph of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b077a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create activation\n",
    "sig = Sigmoid()\n",
    "\n",
    "# Plot function for some values.\n",
    "show_activation(sig, val_range=(-10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191baef3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3 Hyperbolic Tangent (1 Point)\n",
    "\n",
    "---\n",
    "\n",
    "The hyperbolic tangent (tanh) activation function is defined as $\\tanh: \\mathbb{R} \\to \\mathbb{R}$ with\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}.\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The tanh activation function maps values into the range $(-1, 1)$.\n",
    "\n",
    "The derivative of the tanh function is given by\n",
    "\n",
    "$$\n",
    "    \\frac{\\text{d}}{\\text{d}x}\\tanh(x) = 1 - \\tanh(x)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6113b1de",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3.1 Implementation\n",
    "\n",
    "---\n",
    "\n",
    "Complete the definition of the `Tanh` class in the `modules/activation.py` file.\n",
    "\n",
    "In the `forward` method, apply the tanh activation function to the input componentwise and store the result in the `out` variable that is returned from the method. Save the computed outputs for gradient computation in the backward pass.\n",
    "\n",
    "In the `backward` method, compute the gradient of the loss with respect to the inputs. The method receives the gradient of the loss with respect to the layer outputs. Store the gradient of the loss with respect to the layer inputs in the `in_grad` variable that is returned from the method.\n",
    "\n",
    "Your implementation should be fully vectorized, that is, no loops are allowed.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3.1.1 Test\n",
    "\n",
    "To test your implementation you can run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7fc611",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tanh_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb507473",
   "metadata": {},
   "source": [
    "#### 3.1.2 Graph\n",
    "\n",
    "Let's also have a look at the graph of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe88d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create activation\n",
    "tanh = Tanh()\n",
    "\n",
    "# Plot function for some values.\n",
    "show_activation(tanh, val_range=(-10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c457b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4 Saturating Nonlinearities (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "If you implemented the sigmoid and tanh functions correctly, you should be able to observe how both functions become saturated for inputs that are farther away from zero in both directions. Give a concise description of the problems that can be caused by this behavior when using these functions as activations in neural networks.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e717f8b0",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "*Write your answer here.*\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b056e40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
